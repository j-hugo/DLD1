(torch) chan@chan-desktop:~/Desktop/DLD1$ python train.py --model unet --dataset-type only_tumor
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 256, 256]             640
       BatchNorm2d-2         [-1, 64, 256, 256]             128
              ReLU-3         [-1, 64, 256, 256]               0
            Conv2d-4         [-1, 64, 256, 256]          36,928
       BatchNorm2d-5         [-1, 64, 256, 256]             128
              ReLU-6         [-1, 64, 256, 256]               0
         MaxPool2d-7         [-1, 64, 128, 128]               0
            Conv2d-8        [-1, 128, 128, 128]          73,856
       BatchNorm2d-9        [-1, 128, 128, 128]             256
             ReLU-10        [-1, 128, 128, 128]               0
           Conv2d-11        [-1, 128, 128, 128]         147,584
      BatchNorm2d-12        [-1, 128, 128, 128]             256
             ReLU-13        [-1, 128, 128, 128]               0
        MaxPool2d-14          [-1, 128, 64, 64]               0
           Conv2d-15          [-1, 256, 64, 64]         295,168
      BatchNorm2d-16          [-1, 256, 64, 64]             512
             ReLU-17          [-1, 256, 64, 64]               0
           Conv2d-18          [-1, 256, 64, 64]         590,080
      BatchNorm2d-19          [-1, 256, 64, 64]             512
             ReLU-20          [-1, 256, 64, 64]               0
        MaxPool2d-21          [-1, 256, 32, 32]               0
           Conv2d-22          [-1, 512, 32, 32]       1,180,160
      BatchNorm2d-23          [-1, 512, 32, 32]           1,024
             ReLU-24          [-1, 512, 32, 32]               0
           Conv2d-25          [-1, 512, 32, 32]       2,359,808
      BatchNorm2d-26          [-1, 512, 32, 32]           1,024
             ReLU-27          [-1, 512, 32, 32]               0
        MaxPool2d-28          [-1, 512, 16, 16]               0
           Conv2d-29         [-1, 1024, 16, 16]       4,719,616
      BatchNorm2d-30         [-1, 1024, 16, 16]           2,048
             ReLU-31         [-1, 1024, 16, 16]               0
           Conv2d-32         [-1, 1024, 16, 16]       9,438,208
      BatchNorm2d-33         [-1, 1024, 16, 16]           2,048
             ReLU-34         [-1, 1024, 16, 16]               0
         Upsample-35         [-1, 1024, 32, 32]               0
           Conv2d-36          [-1, 512, 32, 32]       7,078,400
             ReLU-37          [-1, 512, 32, 32]               0
           Conv2d-38          [-1, 512, 32, 32]       2,359,808
             ReLU-39          [-1, 512, 32, 32]               0
         Upsample-40          [-1, 512, 64, 64]               0
           Conv2d-41          [-1, 256, 64, 64]       1,769,728
             ReLU-42          [-1, 256, 64, 64]               0
           Conv2d-43          [-1, 256, 64, 64]         590,080
             ReLU-44          [-1, 256, 64, 64]               0
         Upsample-45        [-1, 256, 128, 128]               0
           Conv2d-46        [-1, 128, 128, 128]         442,496
             ReLU-47        [-1, 128, 128, 128]               0
           Conv2d-48        [-1, 128, 128, 128]         147,584
             ReLU-49        [-1, 128, 128, 128]               0
         Upsample-50        [-1, 128, 256, 256]               0
           Conv2d-51         [-1, 64, 256, 256]         110,656
             ReLU-52         [-1, 64, 256, 256]               0
           Conv2d-53         [-1, 64, 256, 256]          36,928
             ReLU-54         [-1, 64, 256, 256]               0
           Conv2d-55          [-1, 1, 256, 256]              65
================================================================
Total params: 31,385,729
Trainable params: 31,385,729
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.25
Forward/backward pass size (MB): 747.50
Params size (MB): 119.73
Estimated Total Size (MB): 867.48
----------------------------------------------------------------
----------------------------------------------------------------
The number of train set: 1032
The number of valid set: 120
----------------------------------------------------------------
Epoch 0/299
----------
LR 0.003
train: bce: 0.186863, dice: 0.988873, loss: 0.186863
val: bce: 0.039108, dice: 0.981269, loss: 0.039108
Validation loss decreased (inf --> 0.039108).  Saving best model ...
0m 40s
Epoch 1/299
----------
LR 0.003
train: bce: 0.034185, dice: 0.978569, loss: 0.034185
val: bce: 0.035174, dice: 0.975067, loss: 0.035174
Validation loss decreased (0.039108 --> 0.035174).  Saving best model ...
0m 40s
Epoch 2/299
----------
LR 0.003
train: bce: 0.033075, dice: 0.975155, loss: 0.033075
val: bce: 0.034592, dice: 0.971922, loss: 0.034592
Validation loss decreased (0.035174 --> 0.034592).  Saving best model ...
0m 41s
Epoch 3/299
----------
LR 0.003
train: bce: 0.032854, dice: 0.974030, loss: 0.032854
val: bce: 0.034473, dice: 0.971875, loss: 0.034473
Validation loss decreased (0.034592 --> 0.034473).  Saving best model ...
0m 41s
Epoch 4/299
----------
LR 0.003
train: bce: 0.032522, dice: 0.972983, loss: 0.032522
val: bce: 0.034124, dice: 0.970329, loss: 0.034124
Validation loss decreased (0.034473 --> 0.034124).  Saving best model ...
0m 41s
Epoch 5/299
----------
LR 0.003
train: bce: 0.032317, dice: 0.971809, loss: 0.032317
val: bce: 0.033679, dice: 0.969111, loss: 0.033679
Validation loss decreased (0.034124 --> 0.033679).  Saving best model ...
0m 42s
Epoch 6/299
----------
LR 0.003
train: bce: 0.032240, dice: 0.971239, loss: 0.032240
val: bce: 0.033678, dice: 0.968589, loss: 0.033678
Validation loss decreased (0.033679 --> 0.033678).  Saving best model ...
0m 41s
Epoch 7/299
----------
LR 0.003
train: bce: 0.032021, dice: 0.970741, loss: 0.032021
val: bce: 0.033828, dice: 0.968808, loss: 0.033828
EarlyStopping counter: 1 out of 15
0m 41s
Epoch 8/299
----------
LR 0.003
train: bce: 0.031908, dice: 0.970768, loss: 0.031908
val: bce: 0.033684, dice: 0.967029, loss: 0.033684
EarlyStopping counter: 2 out of 15
0m 41s
Epoch 9/299
----------
LR 0.003
train: bce: 0.031820, dice: 0.969302, loss: 0.031820
val: bce: 0.033164, dice: 0.966171, loss: 0.033164
Validation loss decreased (0.033678 --> 0.033164).  Saving best model ...
0m 41s
Epoch 10/299
----------
LR 0.003
train: bce: 0.031753, dice: 0.969627, loss: 0.031753
val: bce: 0.033119, dice: 0.965761, loss: 0.033119
Validation loss decreased (0.033164 --> 0.033119).  Saving best model ...
0m 41s
Epoch 11/299
----------
LR 0.003
train: bce: 0.031649, dice: 0.968480, loss: 0.031649
val: bce: 0.032994, dice: 0.966208, loss: 0.032994
Validation loss decreased (0.033119 --> 0.032994).  Saving best model ...
0m 41s
Epoch 12/299
----------
LR 0.003
train: bce: 0.031535, dice: 0.968325, loss: 0.031535
val: bce: 0.032943, dice: 0.966895, loss: 0.032943
Validation loss decreased (0.032994 --> 0.032943).  Saving best model ...
0m 41s
Epoch 13/299
----------
LR 0.003
train: bce: 0.031514, dice: 0.968138, loss: 0.031514
val: bce: 0.032862, dice: 0.962971, loss: 0.032862
Validation loss decreased (0.032943 --> 0.032862).  Saving best model ...
0m 41s
Epoch 14/299
----------
LR 0.003
train: bce: 0.031438, dice: 0.967839, loss: 0.031438
val: bce: 0.032710, dice: 0.965136, loss: 0.032710
Validation loss decreased (0.032862 --> 0.032710).  Saving best model ...
0m 41s
Epoch 15/299
----------
LR 0.003
train: bce: 0.031385, dice: 0.967177, loss: 0.031385
val: bce: 0.032613, dice: 0.965696, loss: 0.032613
Validation loss decreased (0.032710 --> 0.032613).  Saving best model ...
0m 41s
Epoch 16/299
----------
LR 0.003
train: bce: 0.031283, dice: 0.967132, loss: 0.031283
val: bce: 0.032486, dice: 0.962166, loss: 0.032486
Validation loss decreased (0.032613 --> 0.032486).  Saving best model ...
0m 40s
Epoch 17/299
----------
LR 0.003
train: bce: 0.031180, dice: 0.966252, loss: 0.031180
val: bce: 0.032414, dice: 0.965131, loss: 0.032414
Validation loss decreased (0.032486 --> 0.032414).  Saving best model ...
0m 39s
Epoch 18/299
----------
LR 0.003
train: bce: 0.031223, dice: 0.966525, loss: 0.031223
val: bce: 0.032345, dice: 0.963949, loss: 0.032345
Validation loss decreased (0.032414 --> 0.032345).  Saving best model ...
0m 39s
Epoch 19/299
----------
LR 0.003
train: bce: 0.031096, dice: 0.966205, loss: 0.031096
val: bce: 0.032309, dice: 0.960703, loss: 0.032309
Validation loss decreased (0.032345 --> 0.032309).  Saving best model ...
0m 39s
Epoch 20/299
----------
LR 0.003
train: bce: 0.031035, dice: 0.965795, loss: 0.031035
val: bce: 0.032264, dice: 0.958759, loss: 0.032264
Validation loss decreased (0.032309 --> 0.032264).  Saving best model ...
0m 39s
Epoch 21/299
----------
LR 0.003
train: bce: 0.030957, dice: 0.965190, loss: 0.030957
val: bce: 0.031992, dice: 0.959126, loss: 0.031992
Validation loss decreased (0.032264 --> 0.031992).  Saving best model ...
0m 39s
Epoch 22/299
----------
LR 0.003
train: bce: 0.030861, dice: 0.964906, loss: 0.030861
val: bce: 0.031977, dice: 0.960926, loss: 0.031977
Validation loss decreased (0.031992 --> 0.031977).  Saving best model ...
0m 39s
Epoch 23/299
----------
LR 0.003
train: bce: 0.030848, dice: 0.964272, loss: 0.030848
val: bce: 0.031968, dice: 0.963813, loss: 0.031968
Validation loss decreased (0.031977 --> 0.031968).  Saving best model ...
0m 39s
Epoch 24/299
----------
LR 0.003
train: bce: 0.030574, dice: 0.963388, loss: 0.030574
val: bce: 0.031932, dice: 0.964612, loss: 0.031932
Validation loss decreased (0.031968 --> 0.031932).  Saving best model ...
0m 39s
Epoch 25/299
----------
LR 0.003
train: bce: 0.030492, dice: 0.963073, loss: 0.030492
val: bce: 0.031393, dice: 0.956573, loss: 0.031393
Validation loss decreased (0.031932 --> 0.031393).  Saving best model ...
0m 39s
Epoch 26/299
----------
LR 0.003
train: bce: 0.030100, dice: 0.961240, loss: 0.030100
val: bce: 0.030982, dice: 0.953127, loss: 0.030982
Validation loss decreased (0.031393 --> 0.030982).  Saving best model ...
0m 39s
Epoch 27/299
----------
LR 0.003
train: bce: 0.029673, dice: 0.958001, loss: 0.029673
val: bce: 0.030386, dice: 0.950063, loss: 0.030386
Validation loss decreased (0.030982 --> 0.030386).  Saving best model ...
0m 39s
Epoch 28/299
----------
LR 0.003
train: bce: 0.029118, dice: 0.954673, loss: 0.029118
val: bce: 0.029473, dice: 0.945267, loss: 0.029473
Validation loss decreased (0.030386 --> 0.029473).  Saving best model ...
0m 39s
Epoch 29/299
----------
LR 0.003
train: bce: 0.028472, dice: 0.950034, loss: 0.028472
val: bce: 0.029047, dice: 0.940624, loss: 0.029047
Validation loss decreased (0.029473 --> 0.029047).  Saving best model ...
0m 39s
Epoch 30/299
----------
LR 0.003
train: bce: 0.028016, dice: 0.944448, loss: 0.028016
val: bce: 0.028453, dice: 0.935337, loss: 0.028453
Validation loss decreased (0.029047 --> 0.028453).  Saving best model ...
0m 39s
Epoch 31/299
----------
LR 0.003
train: bce: 0.027550, dice: 0.940665, loss: 0.027550
val: bce: 0.027984, dice: 0.927250, loss: 0.027984
Validation loss decreased (0.028453 --> 0.027984).  Saving best model ...
0m 39s
Epoch 32/299
----------
LR 0.003
train: bce: 0.027142, dice: 0.936005, loss: 0.027142
val: bce: 0.027219, dice: 0.925795, loss: 0.027219
Validation loss decreased (0.027984 --> 0.027219).  Saving best model ...
0m 39s
Epoch 33/299
----------
LR 0.003
train: bce: 0.026565, dice: 0.931581, loss: 0.026565
val: bce: 0.027357, dice: 0.910070, loss: 0.027357
EarlyStopping counter: 1 out of 15
0m 39s
Epoch 34/299
----------
LR 0.003
train: bce: 0.025728, dice: 0.921964, loss: 0.025728
val: bce: 0.025524, dice: 0.907567, loss: 0.025524
Validation loss decreased (0.027219 --> 0.025524).  Saving best model ...
0m 39s
Epoch 35/299
----------
LR 0.003
train: bce: 0.024599, dice: 0.910846, loss: 0.024599
val: bce: 0.024715, dice: 0.887647, loss: 0.024715
Validation loss decreased (0.025524 --> 0.024715).  Saving best model ...
0m 39s
Epoch 36/299
----------
LR 0.003
train: bce: 0.023750, dice: 0.896916, loss: 0.023750
val: bce: 0.023663, dice: 0.873787, loss: 0.023663
Validation loss decreased (0.024715 --> 0.023663).  Saving best model ...
0m 39s
Epoch 37/299
----------
LR 0.003
train: bce: 0.023026, dice: 0.885123, loss: 0.023026
val: bce: 0.025434, dice: 0.858130, loss: 0.025434
EarlyStopping counter: 1 out of 15
0m 39s
Epoch 38/299
----------
LR 0.003
train: bce: 0.021941, dice: 0.868096, loss: 0.021941
val: bce: 0.023109, dice: 0.842767, loss: 0.023109
Validation loss decreased (0.023663 --> 0.023109).  Saving best model ...
0m 39s
Epoch 39/299
----------
LR 0.003
train: bce: 0.020952, dice: 0.851889, loss: 0.020952
val: bce: 0.020317, dice: 0.823618, loss: 0.020317
Validation loss decreased (0.023109 --> 0.020317).  Saving best model ...
0m 39s
Epoch 40/299
----------
LR 0.003
train: bce: 0.020136, dice: 0.837039, loss: 0.020136
val: bce: 0.020918, dice: 0.815455, loss: 0.020918
EarlyStopping counter: 1 out of 15
0m 39s
Epoch 41/299
----------
LR 0.003
train: bce: 0.018894, dice: 0.817990, loss: 0.018894
val: bce: 0.019308, dice: 0.803067, loss: 0.019308
Validation loss decreased (0.020317 --> 0.019308).  Saving best model ...
0m 39s
Epoch 42/299
----------
LR 0.003
train: bce: 0.018793, dice: 0.808188, loss: 0.018793
val: bce: 0.020522, dice: 0.825597, loss: 0.020522
EarlyStopping counter: 1 out of 15
0m 39s
Epoch 43/299
----------
LR 0.003
train: bce: 0.018512, dice: 0.806087, loss: 0.018512
val: bce: 0.019229, dice: 0.794524, loss: 0.019229
Validation loss decreased (0.019308 --> 0.019229).  Saving best model ...
0m 39s
Epoch 44/299
----------
LR 0.003
train: bce: 0.018002, dice: 0.792870, loss: 0.018002
val: bce: 0.017143, dice: 0.759786, loss: 0.017143
Validation loss decreased (0.019229 --> 0.017143).  Saving best model ...
0m 39s
Epoch 45/299
----------
LR 0.003
train: bce: 0.016793, dice: 0.777946, loss: 0.016793
val: bce: 0.016804, dice: 0.756986, loss: 0.016804
Validation loss decreased (0.017143 --> 0.016804).  Saving best model ...
0m 39s
Epoch 46/299
----------
LR 0.003
train: bce: 0.016390, dice: 0.766963, loss: 0.016390
val: bce: 0.016954, dice: 0.759230, loss: 0.016954
EarlyStopping counter: 1 out of 15
0m 39s
Epoch 47/299
----------
LR 0.003
train: bce: 0.016761, dice: 0.769945, loss: 0.016761
val: bce: 0.017202, dice: 0.749239, loss: 0.017202
EarlyStopping counter: 2 out of 15
0m 39s
Epoch 48/299
----------
LR 0.003
train: bce: 0.016109, dice: 0.755869, loss: 0.016109
val: bce: 0.018133, dice: 0.748631, loss: 0.018133
EarlyStopping counter: 3 out of 15
0m 39s
Epoch 49/299
----------
LR 0.003
train: bce: 0.015383, dice: 0.742516, loss: 0.015383
val: bce: 0.015201, dice: 0.713010, loss: 0.015201
Validation loss decreased (0.016804 --> 0.015201).  Saving best model ...
0m 39s
Epoch 50/299
----------
LR 0.003
train: bce: 0.014604, dice: 0.732082, loss: 0.014604
val: bce: 0.018896, dice: 0.756049, loss: 0.018896
EarlyStopping counter: 1 out of 15
0m 39s
Epoch 51/299
----------
LR 0.003
train: bce: 0.014533, dice: 0.727115, loss: 0.014533
val: bce: 0.014946, dice: 0.702912, loss: 0.014946
Validation loss decreased (0.015201 --> 0.014946).  Saving best model ...
0m 39s
Epoch 52/299
----------
LR 0.003
train: bce: 0.014357, dice: 0.722282, loss: 0.014357
val: bce: 0.015169, dice: 0.706926, loss: 0.015169
EarlyStopping counter: 1 out of 15
0m 39s
Epoch 53/299
----------
LR 0.003
train: bce: 0.013769, dice: 0.711078, loss: 0.013769
val: bce: 0.016066, dice: 0.701592, loss: 0.016066
EarlyStopping counter: 2 out of 15
0m 39s
Epoch 54/299
----------
LR 0.003
train: bce: 0.013873, dice: 0.707782, loss: 0.013873
val: bce: 0.016027, dice: 0.727588, loss: 0.016027
EarlyStopping counter: 3 out of 15
0m 40s
Epoch 55/299
----------
LR 0.003
train: bce: 0.013355, dice: 0.699793, loss: 0.013355
val: bce: 0.014053, dice: 0.679744, loss: 0.014053
Validation loss decreased (0.014946 --> 0.014053).  Saving best model ...
0m 43s
Epoch 56/299
----------
LR 0.003
train: bce: 0.012990, dice: 0.691259, loss: 0.012990
val: bce: 0.013099, dice: 0.670022, loss: 0.013099
Validation loss decreased (0.014053 --> 0.013099).  Saving best model ...
0m 42s
Epoch 57/299
----------
LR 0.003
train: bce: 0.013051, dice: 0.689818, loss: 0.013051
val: bce: 0.013007, dice: 0.663423, loss: 0.013007
Validation loss decreased (0.013099 --> 0.013007).  Saving best model ...
0m 41s
Epoch 58/299
----------
LR 0.003
train: bce: 0.012722, dice: 0.682764, loss: 0.012722
val: bce: 0.013567, dice: 0.670767, loss: 0.013567
EarlyStopping counter: 1 out of 15
0m 40s
Epoch 59/299
----------
LR 0.003
train: bce: 0.012536, dice: 0.678497, loss: 0.012536
val: bce: 0.014761, dice: 0.672268, loss: 0.014761
EarlyStopping counter: 2 out of 15
0m 40s
Epoch 60/299
----------
LR 0.003
train: bce: 0.012224, dice: 0.668774, loss: 0.012224
val: bce: 0.012993, dice: 0.647250, loss: 0.012993
Validation loss decreased (0.013007 --> 0.012993).  Saving best model ...
0m 41s
Epoch 61/299
----------
LR 0.003
train: bce: 0.012394, dice: 0.669691, loss: 0.012394
val: bce: 0.012955, dice: 0.652000, loss: 0.012955
Validation loss decreased (0.012993 --> 0.012955).  Saving best model ...
0m 41s
Epoch 62/299
----------
LR 0.003
train: bce: 0.011997, dice: 0.661301, loss: 0.011997
val: bce: 0.012834, dice: 0.638373, loss: 0.012834
Validation loss decreased (0.012955 --> 0.012834).  Saving best model ...
0m 41s
Epoch 63/299
----------
LR 0.003
train: bce: 0.012326, dice: 0.661002, loss: 0.012326
val: bce: 0.012419, dice: 0.648501, loss: 0.012419
Validation loss decreased (0.012834 --> 0.012419).  Saving best model ...
0m 42s
Epoch 64/299
----------
LR 0.003
train: bce: 0.011980, dice: 0.656960, loss: 0.011980
val: bce: 0.012633, dice: 0.633868, loss: 0.012633
EarlyStopping counter: 1 out of 15
0m 41s
Epoch 65/299
----------
LR 0.003
train: bce: 0.012061, dice: 0.655012, loss: 0.012061
val: bce: 0.012166, dice: 0.624811, loss: 0.012166
Validation loss decreased (0.012419 --> 0.012166).  Saving best model ...
0m 41s
Epoch 66/299
----------
LR 0.003
train: bce: 0.011817, dice: 0.649763, loss: 0.011817
val: bce: 0.012890, dice: 0.641148, loss: 0.012890
EarlyStopping counter: 1 out of 15
0m 41s
Epoch 67/299
----------------------------------------------------------------
The number of test set: 1189
----------------------------------------------------------------
----------
The Evaluation Starts ...
----------
The total samples: 1189
The average dice score is tensor([[0.8806]], device='cuda:0').
The number of tumor samples: 142
The average dice score of the slices which have tumor is tensor([[4.2806e-11]], device='cuda:0').
The number of non-tumor samples: 1047
The average dice score of the slices which have non-tumor is tensor([[1.]], device='cuda:0').
0m 28s
