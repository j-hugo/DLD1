(torch) chan@chan-desktop:~/Desktop/DLD1$ python train.py --model resnetunet --lr 0.03 --dataset-type only_tumor --epochs 300 --freeze True
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 256, 256]             640
              ReLU-2         [-1, 64, 256, 256]               0
            Conv2d-3         [-1, 64, 256, 256]          36,928
              ReLU-4         [-1, 64, 256, 256]               0
            Conv2d-5         [-1, 64, 128, 128]           3,136
            Conv2d-6         [-1, 64, 128, 128]           3,136
       BatchNorm2d-7         [-1, 64, 128, 128]             128
       BatchNorm2d-8         [-1, 64, 128, 128]             128
              ReLU-9         [-1, 64, 128, 128]               0
             ReLU-10         [-1, 64, 128, 128]               0
        MaxPool2d-11           [-1, 64, 64, 64]               0
        MaxPool2d-12           [-1, 64, 64, 64]               0
           Conv2d-13           [-1, 64, 64, 64]          36,864
           Conv2d-14           [-1, 64, 64, 64]          36,864
      BatchNorm2d-15           [-1, 64, 64, 64]             128
      BatchNorm2d-16           [-1, 64, 64, 64]             128
             ReLU-17           [-1, 64, 64, 64]               0
             ReLU-18           [-1, 64, 64, 64]               0
           Conv2d-19           [-1, 64, 64, 64]          36,864
           Conv2d-20           [-1, 64, 64, 64]          36,864
      BatchNorm2d-21           [-1, 64, 64, 64]             128
      BatchNorm2d-22           [-1, 64, 64, 64]             128
             ReLU-23           [-1, 64, 64, 64]               0
             ReLU-24           [-1, 64, 64, 64]               0
       BasicBlock-25           [-1, 64, 64, 64]               0
       BasicBlock-26           [-1, 64, 64, 64]               0
           Conv2d-27           [-1, 64, 64, 64]          36,864
           Conv2d-28           [-1, 64, 64, 64]          36,864
      BatchNorm2d-29           [-1, 64, 64, 64]             128
      BatchNorm2d-30           [-1, 64, 64, 64]             128
             ReLU-31           [-1, 64, 64, 64]               0
             ReLU-32           [-1, 64, 64, 64]               0
           Conv2d-33           [-1, 64, 64, 64]          36,864
           Conv2d-34           [-1, 64, 64, 64]          36,864
      BatchNorm2d-35           [-1, 64, 64, 64]             128
      BatchNorm2d-36           [-1, 64, 64, 64]             128
             ReLU-37           [-1, 64, 64, 64]               0
             ReLU-38           [-1, 64, 64, 64]               0
       BasicBlock-39           [-1, 64, 64, 64]               0
       BasicBlock-40           [-1, 64, 64, 64]               0
           Conv2d-41           [-1, 64, 64, 64]          36,864
           Conv2d-42           [-1, 64, 64, 64]          36,864
      BatchNorm2d-43           [-1, 64, 64, 64]             128
      BatchNorm2d-44           [-1, 64, 64, 64]             128
             ReLU-45           [-1, 64, 64, 64]               0
             ReLU-46           [-1, 64, 64, 64]               0
           Conv2d-47           [-1, 64, 64, 64]          36,864
           Conv2d-48           [-1, 64, 64, 64]          36,864
      BatchNorm2d-49           [-1, 64, 64, 64]             128
      BatchNorm2d-50           [-1, 64, 64, 64]             128
             ReLU-51           [-1, 64, 64, 64]               0
             ReLU-52           [-1, 64, 64, 64]               0
       BasicBlock-53           [-1, 64, 64, 64]               0
       BasicBlock-54           [-1, 64, 64, 64]               0
           Conv2d-55          [-1, 128, 32, 32]          73,728
           Conv2d-56          [-1, 128, 32, 32]          73,728
      BatchNorm2d-57          [-1, 128, 32, 32]             256
      BatchNorm2d-58          [-1, 128, 32, 32]             256
             ReLU-59          [-1, 128, 32, 32]               0
             ReLU-60          [-1, 128, 32, 32]               0
           Conv2d-61          [-1, 128, 32, 32]         147,456
           Conv2d-62          [-1, 128, 32, 32]         147,456
      BatchNorm2d-63          [-1, 128, 32, 32]             256
      BatchNorm2d-64          [-1, 128, 32, 32]             256
           Conv2d-65          [-1, 128, 32, 32]           8,192
           Conv2d-66          [-1, 128, 32, 32]           8,192
      BatchNorm2d-67          [-1, 128, 32, 32]             256
      BatchNorm2d-68          [-1, 128, 32, 32]             256
             ReLU-69          [-1, 128, 32, 32]               0
             ReLU-70          [-1, 128, 32, 32]               0
       BasicBlock-71          [-1, 128, 32, 32]               0
       BasicBlock-72          [-1, 128, 32, 32]               0
           Conv2d-73          [-1, 128, 32, 32]         147,456
           Conv2d-74          [-1, 128, 32, 32]         147,456
      BatchNorm2d-75          [-1, 128, 32, 32]             256
      BatchNorm2d-76          [-1, 128, 32, 32]             256
             ReLU-77          [-1, 128, 32, 32]               0
             ReLU-78          [-1, 128, 32, 32]               0
           Conv2d-79          [-1, 128, 32, 32]         147,456
           Conv2d-80          [-1, 128, 32, 32]         147,456
      BatchNorm2d-81          [-1, 128, 32, 32]             256
      BatchNorm2d-82          [-1, 128, 32, 32]             256
             ReLU-83          [-1, 128, 32, 32]               0
             ReLU-84          [-1, 128, 32, 32]               0
       BasicBlock-85          [-1, 128, 32, 32]               0
       BasicBlock-86          [-1, 128, 32, 32]               0
           Conv2d-87          [-1, 128, 32, 32]         147,456
           Conv2d-88          [-1, 128, 32, 32]         147,456
      BatchNorm2d-89          [-1, 128, 32, 32]             256
      BatchNorm2d-90          [-1, 128, 32, 32]             256
             ReLU-91          [-1, 128, 32, 32]               0
             ReLU-92          [-1, 128, 32, 32]               0
           Conv2d-93          [-1, 128, 32, 32]         147,456
           Conv2d-94          [-1, 128, 32, 32]         147,456
      BatchNorm2d-95          [-1, 128, 32, 32]             256
      BatchNorm2d-96          [-1, 128, 32, 32]             256
             ReLU-97          [-1, 128, 32, 32]               0
             ReLU-98          [-1, 128, 32, 32]               0
       BasicBlock-99          [-1, 128, 32, 32]               0
      BasicBlock-100          [-1, 128, 32, 32]               0
          Conv2d-101          [-1, 128, 32, 32]         147,456
          Conv2d-102          [-1, 128, 32, 32]         147,456
     BatchNorm2d-103          [-1, 128, 32, 32]             256
     BatchNorm2d-104          [-1, 128, 32, 32]             256
            ReLU-105          [-1, 128, 32, 32]               0
            ReLU-106          [-1, 128, 32, 32]               0
          Conv2d-107          [-1, 128, 32, 32]         147,456
          Conv2d-108          [-1, 128, 32, 32]         147,456
     BatchNorm2d-109          [-1, 128, 32, 32]             256
     BatchNorm2d-110          [-1, 128, 32, 32]             256
            ReLU-111          [-1, 128, 32, 32]               0
            ReLU-112          [-1, 128, 32, 32]               0
      BasicBlock-113          [-1, 128, 32, 32]               0
      BasicBlock-114          [-1, 128, 32, 32]               0
          Conv2d-115          [-1, 256, 16, 16]         294,912
          Conv2d-116          [-1, 256, 16, 16]         294,912
     BatchNorm2d-117          [-1, 256, 16, 16]             512
     BatchNorm2d-118          [-1, 256, 16, 16]             512
            ReLU-119          [-1, 256, 16, 16]               0
            ReLU-120          [-1, 256, 16, 16]               0
          Conv2d-121          [-1, 256, 16, 16]         589,824
          Conv2d-122          [-1, 256, 16, 16]         589,824
     BatchNorm2d-123          [-1, 256, 16, 16]             512
     BatchNorm2d-124          [-1, 256, 16, 16]             512
          Conv2d-125          [-1, 256, 16, 16]          32,768
          Conv2d-126          [-1, 256, 16, 16]          32,768
     BatchNorm2d-127          [-1, 256, 16, 16]             512
     BatchNorm2d-128          [-1, 256, 16, 16]             512
            ReLU-129          [-1, 256, 16, 16]               0
            ReLU-130          [-1, 256, 16, 16]               0
      BasicBlock-131          [-1, 256, 16, 16]               0
      BasicBlock-132          [-1, 256, 16, 16]               0
          Conv2d-133          [-1, 256, 16, 16]         589,824
          Conv2d-134          [-1, 256, 16, 16]         589,824
     BatchNorm2d-135          [-1, 256, 16, 16]             512
     BatchNorm2d-136          [-1, 256, 16, 16]             512
            ReLU-137          [-1, 256, 16, 16]               0
            ReLU-138          [-1, 256, 16, 16]               0
          Conv2d-139          [-1, 256, 16, 16]         589,824
          Conv2d-140          [-1, 256, 16, 16]         589,824
     BatchNorm2d-141          [-1, 256, 16, 16]             512
     BatchNorm2d-142          [-1, 256, 16, 16]             512
            ReLU-143          [-1, 256, 16, 16]               0
            ReLU-144          [-1, 256, 16, 16]               0
      BasicBlock-145          [-1, 256, 16, 16]               0
      BasicBlock-146          [-1, 256, 16, 16]               0
          Conv2d-147          [-1, 256, 16, 16]         589,824
          Conv2d-148          [-1, 256, 16, 16]         589,824
     BatchNorm2d-149          [-1, 256, 16, 16]             512
     BatchNorm2d-150          [-1, 256, 16, 16]             512
            ReLU-151          [-1, 256, 16, 16]               0
            ReLU-152          [-1, 256, 16, 16]               0
          Conv2d-153          [-1, 256, 16, 16]         589,824
          Conv2d-154          [-1, 256, 16, 16]         589,824
     BatchNorm2d-155          [-1, 256, 16, 16]             512
     BatchNorm2d-156          [-1, 256, 16, 16]             512
            ReLU-157          [-1, 256, 16, 16]               0
            ReLU-158          [-1, 256, 16, 16]               0
      BasicBlock-159          [-1, 256, 16, 16]               0
      BasicBlock-160          [-1, 256, 16, 16]               0
          Conv2d-161          [-1, 256, 16, 16]         589,824
          Conv2d-162          [-1, 256, 16, 16]         589,824
     BatchNorm2d-163          [-1, 256, 16, 16]             512
     BatchNorm2d-164          [-1, 256, 16, 16]             512
            ReLU-165          [-1, 256, 16, 16]               0
            ReLU-166          [-1, 256, 16, 16]               0
          Conv2d-167          [-1, 256, 16, 16]         589,824
          Conv2d-168          [-1, 256, 16, 16]         589,824
     BatchNorm2d-169          [-1, 256, 16, 16]             512
     BatchNorm2d-170          [-1, 256, 16, 16]             512
            ReLU-171          [-1, 256, 16, 16]               0
            ReLU-172          [-1, 256, 16, 16]               0
      BasicBlock-173          [-1, 256, 16, 16]               0
      BasicBlock-174          [-1, 256, 16, 16]               0
          Conv2d-175          [-1, 256, 16, 16]         589,824
          Conv2d-176          [-1, 256, 16, 16]         589,824
     BatchNorm2d-177          [-1, 256, 16, 16]             512
     BatchNorm2d-178          [-1, 256, 16, 16]             512
            ReLU-179          [-1, 256, 16, 16]               0
            ReLU-180          [-1, 256, 16, 16]               0
          Conv2d-181          [-1, 256, 16, 16]         589,824
          Conv2d-182          [-1, 256, 16, 16]         589,824
     BatchNorm2d-183          [-1, 256, 16, 16]             512
     BatchNorm2d-184          [-1, 256, 16, 16]             512
            ReLU-185          [-1, 256, 16, 16]               0
            ReLU-186          [-1, 256, 16, 16]               0
      BasicBlock-187          [-1, 256, 16, 16]               0
      BasicBlock-188          [-1, 256, 16, 16]               0
          Conv2d-189          [-1, 256, 16, 16]         589,824
          Conv2d-190          [-1, 256, 16, 16]         589,824
     BatchNorm2d-191          [-1, 256, 16, 16]             512
     BatchNorm2d-192          [-1, 256, 16, 16]             512
            ReLU-193          [-1, 256, 16, 16]               0
            ReLU-194          [-1, 256, 16, 16]               0
          Conv2d-195          [-1, 256, 16, 16]         589,824
          Conv2d-196          [-1, 256, 16, 16]         589,824
     BatchNorm2d-197          [-1, 256, 16, 16]             512
     BatchNorm2d-198          [-1, 256, 16, 16]             512
            ReLU-199          [-1, 256, 16, 16]               0
            ReLU-200          [-1, 256, 16, 16]               0
      BasicBlock-201          [-1, 256, 16, 16]               0
      BasicBlock-202          [-1, 256, 16, 16]               0
          Conv2d-203            [-1, 512, 8, 8]       1,179,648
          Conv2d-204            [-1, 512, 8, 8]       1,179,648
     BatchNorm2d-205            [-1, 512, 8, 8]           1,024
     BatchNorm2d-206            [-1, 512, 8, 8]           1,024
            ReLU-207            [-1, 512, 8, 8]               0
            ReLU-208            [-1, 512, 8, 8]               0
          Conv2d-209            [-1, 512, 8, 8]       2,359,296
          Conv2d-210            [-1, 512, 8, 8]       2,359,296
     BatchNorm2d-211            [-1, 512, 8, 8]           1,024
     BatchNorm2d-212            [-1, 512, 8, 8]           1,024
          Conv2d-213            [-1, 512, 8, 8]         131,072
          Conv2d-214            [-1, 512, 8, 8]         131,072
     BatchNorm2d-215            [-1, 512, 8, 8]           1,024
     BatchNorm2d-216            [-1, 512, 8, 8]           1,024
            ReLU-217            [-1, 512, 8, 8]               0
            ReLU-218            [-1, 512, 8, 8]               0
      BasicBlock-219            [-1, 512, 8, 8]               0
      BasicBlock-220            [-1, 512, 8, 8]               0
          Conv2d-221            [-1, 512, 8, 8]       2,359,296
          Conv2d-222            [-1, 512, 8, 8]       2,359,296
     BatchNorm2d-223            [-1, 512, 8, 8]           1,024
     BatchNorm2d-224            [-1, 512, 8, 8]           1,024
            ReLU-225            [-1, 512, 8, 8]               0
            ReLU-226            [-1, 512, 8, 8]               0
          Conv2d-227            [-1, 512, 8, 8]       2,359,296
          Conv2d-228            [-1, 512, 8, 8]       2,359,296
     BatchNorm2d-229            [-1, 512, 8, 8]           1,024
     BatchNorm2d-230            [-1, 512, 8, 8]           1,024
            ReLU-231            [-1, 512, 8, 8]               0
            ReLU-232            [-1, 512, 8, 8]               0
      BasicBlock-233            [-1, 512, 8, 8]               0
      BasicBlock-234            [-1, 512, 8, 8]               0
          Conv2d-235            [-1, 512, 8, 8]       2,359,296
          Conv2d-236            [-1, 512, 8, 8]       2,359,296
     BatchNorm2d-237            [-1, 512, 8, 8]           1,024
     BatchNorm2d-238            [-1, 512, 8, 8]           1,024
            ReLU-239            [-1, 512, 8, 8]               0
            ReLU-240            [-1, 512, 8, 8]               0
          Conv2d-241            [-1, 512, 8, 8]       2,359,296
          Conv2d-242            [-1, 512, 8, 8]       2,359,296
     BatchNorm2d-243            [-1, 512, 8, 8]           1,024
     BatchNorm2d-244            [-1, 512, 8, 8]           1,024
            ReLU-245            [-1, 512, 8, 8]               0
            ReLU-246            [-1, 512, 8, 8]               0
      BasicBlock-247            [-1, 512, 8, 8]               0
      BasicBlock-248            [-1, 512, 8, 8]               0
          Conv2d-249            [-1, 512, 8, 8]         262,656
            ReLU-250            [-1, 512, 8, 8]               0
        Upsample-251          [-1, 512, 16, 16]               0
          Conv2d-252          [-1, 256, 16, 16]          65,792
            ReLU-253          [-1, 256, 16, 16]               0
          Conv2d-254          [-1, 512, 16, 16]       3,539,456
            ReLU-255          [-1, 512, 16, 16]               0
        Upsample-256          [-1, 512, 32, 32]               0
          Conv2d-257          [-1, 128, 32, 32]          16,512
            ReLU-258          [-1, 128, 32, 32]               0
          Conv2d-259          [-1, 256, 32, 32]       1,474,816
            ReLU-260          [-1, 256, 32, 32]               0
        Upsample-261          [-1, 256, 64, 64]               0
          Conv2d-262           [-1, 64, 64, 64]           4,160
            ReLU-263           [-1, 64, 64, 64]               0
          Conv2d-264          [-1, 256, 64, 64]         737,536
            ReLU-265          [-1, 256, 64, 64]               0
        Upsample-266        [-1, 256, 128, 128]               0
          Conv2d-267         [-1, 64, 128, 128]           4,160
            ReLU-268         [-1, 64, 128, 128]               0
          Conv2d-269        [-1, 128, 128, 128]         368,768
            ReLU-270        [-1, 128, 128, 128]               0
        Upsample-271        [-1, 128, 256, 256]               0
          Conv2d-272         [-1, 64, 256, 256]         110,656
            ReLU-273         [-1, 64, 256, 256]               0
          Conv2d-274          [-1, 2, 256, 256]             130
================================================================
Total params: 49,179,010
Trainable params: 49,179,010
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.25
Forward/backward pass size (MB): 631.00
Params size (MB): 187.60
Estimated Total Size (MB): 818.85
----------------------------------------------------------------
----------------------------------------------------------------
The number of train set: 1072
The number of valid set: 128
----------------------------------------------------------------
Epoch 0/299
----------
LR 0.03
train: bce: 0.121779, dice: 0.511068, loss: 0.663292
val: bce: 0.038394, dice: 0.495245, loss: 0.543237
Validation loss decreased (inf --> 0.543237).  Saving best model ...
0m 21s
Epoch 1/299
----------
LR 0.03
train: bce: 0.038158, dice: 0.492451, loss: 0.540148
val: bce: 0.034739, dice: 0.488108, loss: 0.531532
Validation loss decreased (0.543237 --> 0.531532).  Saving best model ...
0m 21s
Epoch 2/299
----------
LR 0.03
train: bce: 0.037498, dice: 0.479415, loss: 0.526288
val: bce: 0.047861, dice: 0.477506, loss: 0.537333
EarlyStopping counter: 1 out of 15
0m 20s
Epoch 3/299
----------
LR 0.03
train: bce: 0.037746, dice: 0.461926, loss: 0.509109
val: bce: 0.039639, dice: 0.455854, loss: 0.505403
Validation loss decreased (0.531532 --> 0.505403).  Saving best model ...
0m 21s
Epoch 4/299
----------
LR 0.03
train: bce: 0.037071, dice: 0.444558, loss: 0.490897
val: bce: 0.047984, dice: 0.440302, loss: 0.500282
Validation loss decreased (0.505403 --> 0.500282).  Saving best model ...
0m 21s
Epoch 5/299
----------
LR 0.03
train: bce: 0.036333, dice: 0.419489, loss: 0.464905
val: bce: 0.029923, dice: 0.430473, loss: 0.467876
Validation loss decreased (0.500282 --> 0.467876).  Saving best model ...
0m 22s
Epoch 6/299
----------
LR 0.03
train: bce: 0.038365, dice: 0.419785, loss: 0.467741
val: bce: 0.033791, dice: 0.432567, loss: 0.474806
EarlyStopping counter: 1 out of 15
0m 21s
Epoch 7/299
----------
LR 0.03
train: bce: 0.034562, dice: 0.394979, loss: 0.438182
val: bce: 0.034145, dice: 0.409634, loss: 0.452316
Validation loss decreased (0.467876 --> 0.452316).  Saving best model ...
0m 21s
Epoch 8/299
----------
LR 0.03
train: bce: 0.033552, dice: 0.384630, loss: 0.426570
val: bce: 0.031835, dice: 0.410331, loss: 0.450125
Validation loss decreased (0.452316 --> 0.450125).  Saving best model ...
0m 21s
Epoch 9/299
----------
LR 0.03
train: bce: 0.032904, dice: 0.372038, loss: 0.413168
val: bce: 0.040727, dice: 0.386472, loss: 0.437381
Validation loss decreased (0.450125 --> 0.437381).  Saving best model ...
0m 21s
Epoch 10/299
----------
LR 0.03
train: bce: 0.031220, dice: 0.358649, loss: 0.397674
val: bce: 0.030437, dice: 0.362755, loss: 0.400801
Validation loss decreased (0.437381 --> 0.400801).  Saving best model ...
0m 22s
Epoch 11/299
----------
LR 0.03
train: bce: 0.031125, dice: 0.350428, loss: 0.389335
val: bce: 0.030469, dice: 0.374403, loss: 0.412489
EarlyStopping counter: 1 out of 15
0m 22s
Epoch 12/299
----------
LR 0.03
train: bce: 0.029430, dice: 0.335577, loss: 0.372365
val: bce: 0.032938, dice: 0.377650, loss: 0.418823
EarlyStopping counter: 2 out of 15
0m 22s
Epoch 13/299
----------
LR 0.03
train: bce: 0.029376, dice: 0.334738, loss: 0.371458
val: bce: 0.025886, dice: 0.357563, loss: 0.389920
Validation loss decreased (0.400801 --> 0.389920).  Saving best model ...
0m 22s
Epoch 14/299
----------
LR 0.03
train: bce: 0.027900, dice: 0.327302, loss: 0.362177
val: bce: 0.034268, dice: 0.356011, loss: 0.398846
EarlyStopping counter: 1 out of 15
0m 23s
Epoch 15/299
----------
LR 0.03
train: bce: 0.025144, dice: 0.307880, loss: 0.339311
val: bce: 0.026120, dice: 0.357903, loss: 0.390553
EarlyStopping counter: 2 out of 15
0m 23s
Epoch 16/299
----------
LR 0.03
train: bce: 0.026775, dice: 0.312799, loss: 0.346267
val: bce: 0.027509, dice: 0.352799, loss: 0.387186
Validation loss decreased (0.389920 --> 0.387186).  Saving best model ...
0m 23s
Epoch 17/299
----------
LR 0.03
train: bce: 0.026369, dice: 0.306613, loss: 0.339573
val: bce: 0.030300, dice: 0.325511, loss: 0.363387
Validation loss decreased (0.387186 --> 0.363387).  Saving best model ...
0m 24s
Epoch 18/299
----------
LR 0.03
train: bce: 0.025491, dice: 0.295891, loss: 0.327755
val: bce: 0.034743, dice: 0.332773, loss: 0.376202
EarlyStopping counter: 1 out of 15
0m 20s
Epoch 19/299
----------
LR 0.03
train: bce: 0.026200, dice: 0.308025, loss: 0.340775
val: bce: 0.027527, dice: 0.335380, loss: 0.369789
EarlyStopping counter: 2 out of 15
0m 20s
Epoch 20/299
----------
LR 0.03
train: bce: 0.023451, dice: 0.285715, loss: 0.315029
val: bce: 0.031525, dice: 0.328479, loss: 0.367885
EarlyStopping counter: 3 out of 15
0m 20s
Epoch 21/299
----------
LR 0.03
train: bce: 0.023806, dice: 0.286757, loss: 0.316514
val: bce: 0.025369, dice: 0.315492, loss: 0.347204
Validation loss decreased (0.363387 --> 0.347204).  Saving best model ...
0m 21s
Epoch 22/299
----------
LR 0.03
train: bce: 0.023450, dice: 0.284824, loss: 0.314136
val: bce: 0.025368, dice: 0.308497, loss: 0.340207
Validation loss decreased (0.347204 --> 0.340207).  Saving best model ...
0m 22s
Epoch 23/299
----------
LR 0.03
train: bce: 0.022472, dice: 0.275784, loss: 0.303874
val: bce: 0.031987, dice: 0.327571, loss: 0.367555
EarlyStopping counter: 1 out of 15
0m 22s
Epoch 24/299
----------
LR 0.03
train: bce: 0.022228, dice: 0.276495, loss: 0.304279
val: bce: 0.023893, dice: 0.298930, loss: 0.328797
Validation loss decreased (0.340207 --> 0.328797).  Saving best model ...
0m 22s
Epoch 25/299
----------
LR 0.03
train: bce: 0.023943, dice: 0.278327, loss: 0.308255
val: bce: 0.030652, dice: 0.317435, loss: 0.355750
EarlyStopping counter: 1 out of 15
0m 21s
Epoch 26/299
----------
LR 0.03
train: bce: 0.021166, dice: 0.266459, loss: 0.292916
val: bce: 0.040767, dice: 0.320293, loss: 0.371252
EarlyStopping counter: 2 out of 15
0m 21s
Epoch 27/299
----------
LR 0.03
train: bce: 0.021180, dice: 0.262446, loss: 0.288921
val: bce: 0.021878, dice: 0.300276, loss: 0.327623
Validation loss decreased (0.328797 --> 0.327623).  Saving best model ...
0m 21s
Epoch 28/299
----------
LR 0.03
train: bce: 0.020494, dice: 0.255103, loss: 0.280721
val: bce: 0.020077, dice: 0.279468, loss: 0.304565
Validation loss decreased (0.327623 --> 0.304565).  Saving best model ...
0m 21s
Epoch 29/299
----------
LR 0.03
train: bce: 0.021297, dice: 0.261575, loss: 0.288197
val: bce: 0.031008, dice: 0.302472, loss: 0.341232
EarlyStopping counter: 1 out of 15
0m 21s
Epoch 30/299
----------
LR 0.03
train: bce: 0.019955, dice: 0.252691, loss: 0.277636
val: bce: 0.027189, dice: 0.291206, loss: 0.325191
EarlyStopping counter: 2 out of 15
0m 22s
Epoch 31/299
----------
LR 0.03
train: bce: 0.020129, dice: 0.250877, loss: 0.276039
val: bce: 0.025786, dice: 0.304462, loss: 0.336695
EarlyStopping counter: 3 out of 15
0m 21s
Epoch 32/299
----------
LR 0.03
train: bce: 0.020002, dice: 0.249580, loss: 0.274583
val: bce: 0.019679, dice: 0.287438, loss: 0.312036
EarlyStopping counter: 4 out of 15
0m 20s
Epoch 33/299
----------
LR 0.03
train: bce: 0.019917, dice: 0.246792, loss: 0.271688
val: bce: 0.022207, dice: 0.293290, loss: 0.321048
EarlyStopping counter: 5 out of 15
0m 20s
Epoch 34/299
----------
LR 0.03
train: bce: 0.019261, dice: 0.242186, loss: 0.266263
val: bce: 0.026362, dice: 0.295220, loss: 0.328173
EarlyStopping counter: 6 out of 15
0m 20s
Epoch 35/299
----------
LR 0.015
train: bce: 0.018389, dice: 0.232642, loss: 0.255628
val: bce: 0.021180, dice: 0.280623, loss: 0.307098
EarlyStopping counter: 7 out of 15
0m 21s
Epoch 36/299
----------
LR 0.015
train: bce: 0.017658, dice: 0.225168, loss: 0.247241
val: bce: 0.022954, dice: 0.301076, loss: 0.329769
EarlyStopping counter: 8 out of 15
0m 21s
Epoch 37/299
----------
LR 0.015
train: bce: 0.017603, dice: 0.224710, loss: 0.246713
val: bce: 0.020552, dice: 0.269977, loss: 0.295668
Validation loss decreased (0.304565 --> 0.295668).  Saving best model ...
0m 24s
Epoch 38/299
----------
LR 0.015
train: bce: 0.016655, dice: 0.220606, loss: 0.241425
val: bce: 0.021060, dice: 0.267262, loss: 0.293586
Validation loss decreased (0.295668 --> 0.293586).  Saving best model ...
0m 24s
Epoch 39/299
----------
LR 0.015
train: bce: 0.016073, dice: 0.212334, loss: 0.232425
val: bce: 0.021302, dice: 0.269130, loss: 0.295758
EarlyStopping counter: 1 out of 15
0m 21s
Epoch 40/299
----------
LR 0.015
train: bce: 0.017287, dice: 0.223697, loss: 0.245306
val: bce: 0.023066, dice: 0.278330, loss: 0.307162
EarlyStopping counter: 2 out of 15
0m 20s
Epoch 41/299
----------
LR 0.015
train: bce: 0.015859, dice: 0.212819, loss: 0.232643
val: bce: 0.020501, dice: 0.260468, loss: 0.286095
Validation loss decreased (0.293586 --> 0.286095).  Saving best model ...
0m 21s
Epoch 42/299
----------
LR 0.015
train: bce: 0.016363, dice: 0.212601, loss: 0.233055
val: bce: 0.019686, dice: 0.263245, loss: 0.287853
EarlyStopping counter: 1 out of 15
0m 20s
Epoch 43/299
----------
LR 0.015
train: bce: 0.016231, dice: 0.212120, loss: 0.232409
val: bce: 0.027074, dice: 0.265383, loss: 0.299226
EarlyStopping counter: 2 out of 15
0m 20s
Epoch 44/299
----------
LR 0.015
train: bce: 0.016916, dice: 0.216708, loss: 0.237853
val: bce: 0.020399, dice: 0.256435, loss: 0.281933
Validation loss decreased (0.286095 --> 0.281933).  Saving best model ...
0m 21s
Epoch 45/299
----------
LR 0.015
train: bce: 0.015791, dice: 0.207105, loss: 0.226844
val: bce: 0.021644, dice: 0.256185, loss: 0.283240
EarlyStopping counter: 1 out of 15
0m 20s
Epoch 46/299
----------
LR 0.015
train: bce: 0.015157, dice: 0.202950, loss: 0.221896
val: bce: 0.022502, dice: 0.254442, loss: 0.282569
EarlyStopping counter: 2 out of 15
0m 20s
Epoch 47/299
----------
LR 0.015
train: bce: 0.016365, dice: 0.207156, loss: 0.227612
val: bce: 0.019099, dice: 0.252336, loss: 0.276210
Validation loss decreased (0.281933 --> 0.276210).  Saving best model ...
0m 21s
Epoch 48/299
----------
LR 0.015
train: bce: 0.015048, dice: 0.201568, loss: 0.220379
val: bce: 0.019034, dice: 0.241759, loss: 0.265552
Validation loss decreased (0.276210 --> 0.265552).  Saving best model ...
0m 21s
Epoch 49/299
----------
LR 0.015
train: bce: 0.015187, dice: 0.198387, loss: 0.217371
val: bce: 0.019470, dice: 0.244846, loss: 0.269183
EarlyStopping counter: 1 out of 15
0m 20s
Epoch 50/299
----------
LR 0.015
train: bce: 0.015206, dice: 0.198686, loss: 0.217693
val: bce: 0.017989, dice: 0.241620, loss: 0.264106
Validation loss decreased (0.265552 --> 0.264106).  Saving best model ...
0m 21s
Epoch 51/299
----------
LR 0.015
train: bce: 0.014984, dice: 0.196357, loss: 0.215087
val: bce: 0.018207, dice: 0.250871, loss: 0.273629
EarlyStopping counter: 1 out of 15
0m 20s
Epoch 52/299
----------
LR 0.015
train: bce: 0.015533, dice: 0.196481, loss: 0.215897
val: bce: 0.018180, dice: 0.255593, loss: 0.278318
EarlyStopping counter: 2 out of 15
0m 20s
Epoch 53/299
----------
LR 0.015
train: bce: 0.014567, dice: 0.196230, loss: 0.214440
val: bce: 0.019096, dice: 0.263337, loss: 0.287207
EarlyStopping counter: 3 out of 15
0m 20s
Epoch 54/299
----------
LR 0.015
train: bce: 0.015293, dice: 0.196723, loss: 0.215839
val: bce: 0.019650, dice: 0.271409, loss: 0.295971
EarlyStopping counter: 4 out of 15
0m 21s
Epoch 55/299
----------
LR 0.015
train: bce: 0.014772, dice: 0.190699, loss: 0.209164
val: bce: 0.019267, dice: 0.258780, loss: 0.282863
EarlyStopping counter: 5 out of 15
0m 21s
Epoch 56/299
----------
LR 0.015
train: bce: 0.015429, dice: 0.201384, loss: 0.220669
val: bce: 0.018493, dice: 0.242798, loss: 0.265914
EarlyStopping counter: 6 out of 15
0m 21s
Epoch 57/299
----------
LR 0.0075
train: bce: 0.013787, dice: 0.184343, loss: 0.201577
val: bce: 0.016971, dice: 0.241451, loss: 0.262665
Validation loss decreased (0.264106 --> 0.262665).  Saving best model ...
0m 21s
Epoch 58/299
----------
LR 0.0075
train: bce: 0.012834, dice: 0.176741, loss: 0.192783
val: bce: 0.017359, dice: 0.219518, loss: 0.241216
Validation loss decreased (0.262665 --> 0.241216).  Saving best model ...
0m 21s
Epoch 59/299
----------
LR 0.0075
train: bce: 0.013057, dice: 0.175181, loss: 0.191502
val: bce: 0.016879, dice: 0.241114, loss: 0.262213
EarlyStopping counter: 1 out of 15
0m 20s
Epoch 60/299
----------
LR 0.0075
train: bce: 0.012705, dice: 0.172322, loss: 0.188204
val: bce: 0.018254, dice: 0.232540, loss: 0.255357
EarlyStopping counter: 2 out of 15
0m 20s
Epoch 61/299
----------
LR 0.0075
train: bce: 0.012798, dice: 0.169964, loss: 0.185961
val: bce: 0.019358, dice: 0.229534, loss: 0.253732
EarlyStopping counter: 3 out of 15
0m 20s
Epoch 62/299
----------
LR 0.0075
train: bce: 0.013941, dice: 0.180038, loss: 0.197463
val: bce: 0.015923, dice: 0.229433, loss: 0.249337
EarlyStopping counter: 4 out of 15
0m 20s
Epoch 63/299
----------
LR 0.0075
train: bce: 0.012898, dice: 0.170077, loss: 0.186199
val: bce: 0.018037, dice: 0.239229, loss: 0.261775
EarlyStopping counter: 5 out of 15
0m 20s
Epoch 64/299
----------
LR 0.0075
train: bce: 0.012886, dice: 0.173607, loss: 0.189714
val: bce: 0.016634, dice: 0.251917, loss: 0.272710
EarlyStopping counter: 6 out of 15
0m 20s
Epoch 65/299
----------
LR 0.00375
train: bce: 0.011626, dice: 0.164403, loss: 0.178936
val: bce: 0.016823, dice: 0.228762, loss: 0.249791
EarlyStopping counter: 7 out of 15
0m 21s
Epoch 66/299
----------
LR 0.00375
train: bce: 0.011959, dice: 0.165424, loss: 0.180372
val: bce: 0.016474, dice: 0.227116, loss: 0.247709
EarlyStopping counter: 8 out of 15
0m 24s
Epoch 67/299
----------
LR 0.00375
train: bce: 0.012038, dice: 0.164116, loss: 0.179164
val: bce: 0.015739, dice: 0.220814, loss: 0.240488
Validation loss decreased (0.241216 --> 0.240488).  Saving best model ...
0m 22s
Epoch 68/299
----------
LR 0.00375
train: bce: 0.011502, dice: 0.158378, loss: 0.172755
val: bce: 0.015776, dice: 0.216737, loss: 0.236457
Validation loss decreased (0.240488 --> 0.236457).  Saving best model ...
0m 21s
Epoch 69/299
----------
LR 0.00375
train: bce: 0.011849, dice: 0.158330, loss: 0.173141
val: bce: 0.016411, dice: 0.210404, loss: 0.230918
Validation loss decreased (0.236457 --> 0.230918).  Saving best model ...
0m 21s
Epoch 70/299
----------
LR 0.00375
train: bce: 0.012099, dice: 0.161560, loss: 0.176684
val: bce: 0.015379, dice: 0.229588, loss: 0.248812
EarlyStopping counter: 1 out of 15
0m 21s
Epoch 71/299
----------
LR 0.00375
train: bce: 0.011510, dice: 0.159660, loss: 0.174047
val: bce: 0.015636, dice: 0.217928, loss: 0.237473
EarlyStopping counter: 2 out of 15
0m 21s
Epoch 72/299
----------
LR 0.00375
train: bce: 0.011514, dice: 0.158409, loss: 0.172801
val: bce: 0.016377, dice: 0.213720, loss: 0.234191
EarlyStopping counter: 3 out of 15
0m 21s
Epoch 73/299
----------
LR 0.00375
train: bce: 0.012178, dice: 0.161092, loss: 0.176314
val: bce: 0.015907, dice: 0.222820, loss: 0.242704
EarlyStopping counter: 4 out of 15
0m 20s
Epoch 74/299
----------
LR 0.00375
train: bce: 0.011400, dice: 0.157131, loss: 0.171381
val: bce: 0.015538, dice: 0.222697, loss: 0.242120
EarlyStopping counter: 5 out of 15
0m 20s
Epoch 75/299
----------
LR 0.00375
train: bce: 0.011354, dice: 0.156423, loss: 0.170615
val: bce: 0.015034, dice: 0.212165, loss: 0.230958
EarlyStopping counter: 6 out of 15
0m 20s
Epoch 76/299
----------
LR 0.001875
train: bce: 0.011074, dice: 0.154807, loss: 0.168650
val: bce: 0.016416, dice: 0.225190, loss: 0.245710
EarlyStopping counter: 7 out of 15
0m 20s
Epoch 77/299
----------
LR 0.001875
train: bce: 0.011174, dice: 0.150948, loss: 0.164916
val: bce: 0.014970, dice: 0.216690, loss: 0.235402
EarlyStopping counter: 8 out of 15
0m 20s
Epoch 78/299
----------
LR 0.001875
train: bce: 0.011214, dice: 0.154558, loss: 0.168575
val: bce: 0.015260, dice: 0.209637, loss: 0.228712
Validation loss decreased (0.230918 --> 0.228712).  Saving best model ...
0m 21s
Epoch 79/299
----------
LR 0.001875
train: bce: 0.011135, dice: 0.151839, loss: 0.165757
val: bce: 0.014747, dice: 0.209956, loss: 0.228390
Validation loss decreased (0.228712 --> 0.228390).  Saving best model ...
0m 21s
Epoch 80/299
----------
LR 0.001875
train: bce: 0.011075, dice: 0.150117, loss: 0.163960
val: bce: 0.015358, dice: 0.209794, loss: 0.228992
EarlyStopping counter: 1 out of 15
0m 20s
Epoch 81/299
----------
LR 0.001875
train: bce: 0.011204, dice: 0.152547, loss: 0.166553
val: bce: 0.014949, dice: 0.212741, loss: 0.231427
EarlyStopping counter: 2 out of 15
0m 21s
Epoch 82/299
----------
LR 0.001875
train: bce: 0.011105, dice: 0.151706, loss: 0.165587
val: bce: 0.015789, dice: 0.209029, loss: 0.228765
EarlyStopping counter: 3 out of 15
0m 21s
Epoch 83/299
----------
LR 0.001875
train: bce: 0.011528, dice: 0.155734, loss: 0.170143
val: bce: 0.015693, dice: 0.225525, loss: 0.245141
EarlyStopping counter: 4 out of 15
0m 20s
Epoch 84/299
----------
LR 0.001875
train: bce: 0.010935, dice: 0.152280, loss: 0.165949
val: bce: 0.014290, dice: 0.218099, loss: 0.235961
EarlyStopping counter: 5 out of 15
0m 20s
Epoch 85/299
----------
LR 0.001875
train: bce: 0.010950, dice: 0.148662, loss: 0.162349
val: bce: 0.013491, dice: 0.208957, loss: 0.225821
Validation loss decreased (0.228390 --> 0.225821).  Saving best model ...
0m 21s
Epoch 86/299
----------
LR 0.001875
train: bce: 0.010973, dice: 0.148919, loss: 0.162636
val: bce: 0.014959, dice: 0.220209, loss: 0.238908
EarlyStopping counter: 1 out of 15
0m 20s
Epoch 87/299
----------
LR 0.001875
train: bce: 0.010575, dice: 0.147412, loss: 0.160631
val: bce: 0.014760, dice: 0.211030, loss: 0.229479
EarlyStopping counter: 2 out of 15
0m 20s
Epoch 88/299
----------
LR 0.001875
train: bce: 0.010762, dice: 0.146961, loss: 0.160414
val: bce: 0.015761, dice: 0.214183, loss: 0.233884
EarlyStopping counter: 3 out of 15
0m 20s
Epoch 89/299
----------
LR 0.001875
train: bce: 0.010700, dice: 0.148983, loss: 0.162358
val: bce: 0.017220, dice: 0.213553, loss: 0.235078
EarlyStopping counter: 4 out of 15
0m 20s
Epoch 90/299
----------
LR 0.001875
train: bce: 0.010705, dice: 0.148112, loss: 0.161493
val: bce: 0.016112, dice: 0.221407, loss: 0.241547
EarlyStopping counter: 5 out of 15
0m 20s
Epoch 91/299
----------
LR 0.001875
train: bce: 0.010351, dice: 0.142818, loss: 0.155757
val: bce: 0.015858, dice: 0.212010, loss: 0.231833
EarlyStopping counter: 6 out of 15
0m 21s
Epoch 92/299
----------
LR 0.0009375
train: bce: 0.010739, dice: 0.146624, loss: 0.160048
val: bce: 0.016572, dice: 0.215155, loss: 0.235871
EarlyStopping counter: 7 out of 15
0m 20s
Epoch 93/299
----------
LR 0.0009375
train: bce: 0.010406, dice: 0.146331, loss: 0.159339
val: bce: 0.014453, dice: 0.212285, loss: 0.230351
EarlyStopping counter: 8 out of 15
0m 20s
Epoch 94/299
----------
LR 0.0009375
train: bce: 0.010541, dice: 0.144743, loss: 0.157919
val: bce: 0.014470, dice: 0.207081, loss: 0.225169
Validation loss decreased (0.225821 --> 0.225169).  Saving best model ...
0m 21s
Epoch 95/299
----------
LR 0.0009375
train: bce: 0.010515, dice: 0.144989, loss: 0.158133
val: bce: 0.013692, dice: 0.203407, loss: 0.220522
Validation loss decreased (0.225169 --> 0.220522).  Saving best model ...
0m 21s
Epoch 96/299
----------
LR 0.0009375
train: bce: 0.010631, dice: 0.145605, loss: 0.158895
val: bce: 0.016317, dice: 0.208763, loss: 0.229159
EarlyStopping counter: 1 out of 15
0m 20s
Epoch 97/299
----------
LR 0.0009375
train: bce: 0.010527, dice: 0.143783, loss: 0.156941
val: bce: 0.015076, dice: 0.208387, loss: 0.227232
EarlyStopping counter: 2 out of 15
0m 22s
Epoch 98/299
----------
LR 0.0009375
train: bce: 0.010979, dice: 0.147031, loss: 0.160755
val: bce: 0.015281, dice: 0.211040, loss: 0.230142
EarlyStopping counter: 3 out of 15
0m 20s
Epoch 99/299
----------
LR 0.0009375
train: bce: 0.010569, dice: 0.145583, loss: 0.158795
val: bce: 0.014291, dice: 0.196108, loss: 0.213972
Validation loss decreased (0.220522 --> 0.213972).  Saving best model ...
0m 21s
Epoch 100/299
----------
LR 0.0009375
train: bce: 0.010591, dice: 0.142812, loss: 0.156051
val: bce: 0.014428, dice: 0.207998, loss: 0.226033
EarlyStopping counter: 1 out of 15
0m 20s
Epoch 101/299
----------
LR 0.0009375
train: bce: 0.010732, dice: 0.144504, loss: 0.157919
val: bce: 0.015349, dice: 0.213949, loss: 0.233134
EarlyStopping counter: 2 out of 15
0m 20s
Epoch 102/299
----------
LR 0.0009375
train: bce: 0.010431, dice: 0.142891, loss: 0.155929
val: bce: 0.014827, dice: 0.205324, loss: 0.223857
EarlyStopping counter: 3 out of 15
0m 20s
Epoch 103/299
----------
LR 0.0009375
train: bce: 0.010152, dice: 0.140769, loss: 0.153459
val: bce: 0.014185, dice: 0.197143, loss: 0.214874
EarlyStopping counter: 4 out of 15
0m 20s
Epoch 104/299
----------
LR 0.0009375
train: bce: 0.010389, dice: 0.143138, loss: 0.156124
val: bce: 0.015597, dice: 0.211748, loss: 0.231244
EarlyStopping counter: 5 out of 15
0m 20s
Epoch 105/299
----------
LR 0.0009375
train: bce: 0.010455, dice: 0.141491, loss: 0.154560
val: bce: 0.013666, dice: 0.200086, loss: 0.217168
EarlyStopping counter: 6 out of 15
0m 20s
Epoch 106/299
----------
LR 0.00046875
train: bce: 0.010467, dice: 0.142610, loss: 0.155694
val: bce: 0.015019, dice: 0.205313, loss: 0.224087
EarlyStopping counter: 7 out of 15
0m 20s
Epoch 107/299
----------
LR 0.00046875
train: bce: 0.010575, dice: 0.144432, loss: 0.157652
val: bce: 0.015252, dice: 0.209204, loss: 0.228269
EarlyStopping counter: 8 out of 15
0m 20s
Epoch 108/299
----------
LR 0.00046875
train: bce: 0.010543, dice: 0.143150, loss: 0.156329
val: bce: 0.014155, dice: 0.207181, loss: 0.224874
EarlyStopping counter: 9 out of 15
0m 20s
Epoch 109/299
----------
LR 0.00046875
train: bce: 0.010230, dice: 0.141786, loss: 0.154573
val: bce: 0.014475, dice: 0.202848, loss: 0.220943
EarlyStopping counter: 10 out of 15
0m 21s
Epoch 110/299
----------
LR 0.00046875
train: bce: 0.010324, dice: 0.142543, loss: 0.155449
val: bce: 0.014910, dice: 0.209311, loss: 0.227949
EarlyStopping counter: 11 out of 15
0m 21s
Epoch 111/299
----------
LR 0.00046875
train: bce: 0.010348, dice: 0.141933, loss: 0.154868
val: bce: 0.014217, dice: 0.209122, loss: 0.226893
EarlyStopping counter: 12 out of 15
0m 21s
Epoch 112/299
----------
LR 0.000234375
train: bce: 0.010254, dice: 0.142907, loss: 0.155724
val: bce: 0.015266, dice: 0.218074, loss: 0.237157
EarlyStopping counter: 13 out of 15
0m 22s
Epoch 113/299
----------
LR 0.000234375
train: bce: 0.010084, dice: 0.141629, loss: 0.154235
val: bce: 0.015134, dice: 0.213217, loss: 0.232133
EarlyStopping counter: 14 out of 15
0m 22s
Epoch 114/299
----------
LR 0.000234375
train: bce: 0.010085, dice: 0.140705, loss: 0.153312
val: bce: 0.013565, dice: 0.201430, loss: 0.218386
EarlyStopping counter: 15 out of 15
0m 22s
Early stopping
Best val loss: 0.213972

** To the test set **
----------------------------------------------------------------
The number of test set: 1683
----------------------------------------------------------------
----------
The Evaluation Starts ...
----------
The total samples: 1683
The average dice score is 0.24144992232322693.
The number of tumor samples: 108
The average dice score of the slices which have tumor is 0.012594680301845074.
The number of non-tumor samples: 1575
The average dice score of the slices which have non-tumor is 0.2571428418159485.

** To the training set **
----------------------------------------------------------------
The number of training set: 1177
----------------------------------------------------------------
----------
The Evaluation Starts ...
----------
The total samples: 1177
The average dice score is 0.6422142386436462.
The number of tumor samples: 1170
The average dice score of the slices which have tumor is 0.6460565328598022.
The number of non-tumor samples: 7
The average dice score of the slices which have non-tumor is 2.3392696113511136e-10.
0m 24s

