torch) chan@chan-desktop:~/Desktop/DLD1$ python train.py --epochs 200 --lr 0.003 --model resnetunet --freeze True --dataset-type undersample
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 256, 256]             640
              ReLU-2         [-1, 64, 256, 256]               0
            Conv2d-3         [-1, 64, 256, 256]          36,928
              ReLU-4         [-1, 64, 256, 256]               0
            Conv2d-5         [-1, 64, 128, 128]           3,136
            Conv2d-6         [-1, 64, 128, 128]           3,136
       BatchNorm2d-7         [-1, 64, 128, 128]             128
       BatchNorm2d-8         [-1, 64, 128, 128]             128
              ReLU-9         [-1, 64, 128, 128]               0
             ReLU-10         [-1, 64, 128, 128]               0
        MaxPool2d-11           [-1, 64, 64, 64]               0
        MaxPool2d-12           [-1, 64, 64, 64]               0
           Conv2d-13           [-1, 64, 64, 64]          36,864
           Conv2d-14           [-1, 64, 64, 64]          36,864
      BatchNorm2d-15           [-1, 64, 64, 64]             128
      BatchNorm2d-16           [-1, 64, 64, 64]             128
             ReLU-17           [-1, 64, 64, 64]               0
             ReLU-18           [-1, 64, 64, 64]               0
           Conv2d-19           [-1, 64, 64, 64]          36,864
           Conv2d-20           [-1, 64, 64, 64]          36,864
      BatchNorm2d-21           [-1, 64, 64, 64]             128
      BatchNorm2d-22           [-1, 64, 64, 64]             128
             ReLU-23           [-1, 64, 64, 64]               0
             ReLU-24           [-1, 64, 64, 64]               0
       BasicBlock-25           [-1, 64, 64, 64]               0
       BasicBlock-26           [-1, 64, 64, 64]               0
           Conv2d-27           [-1, 64, 64, 64]          36,864
           Conv2d-28           [-1, 64, 64, 64]          36,864
      BatchNorm2d-29           [-1, 64, 64, 64]             128
      BatchNorm2d-30           [-1, 64, 64, 64]             128
             ReLU-31           [-1, 64, 64, 64]               0
             ReLU-32           [-1, 64, 64, 64]               0
           Conv2d-33           [-1, 64, 64, 64]          36,864
           Conv2d-34           [-1, 64, 64, 64]          36,864
      BatchNorm2d-35           [-1, 64, 64, 64]             128
      BatchNorm2d-36           [-1, 64, 64, 64]             128
             ReLU-37           [-1, 64, 64, 64]               0
             ReLU-38           [-1, 64, 64, 64]               0
       BasicBlock-39           [-1, 64, 64, 64]               0
       BasicBlock-40           [-1, 64, 64, 64]               0
           Conv2d-41           [-1, 64, 64, 64]          36,864
           Conv2d-42           [-1, 64, 64, 64]          36,864
      BatchNorm2d-43           [-1, 64, 64, 64]             128
      BatchNorm2d-44           [-1, 64, 64, 64]             128
             ReLU-45           [-1, 64, 64, 64]               0
             ReLU-46           [-1, 64, 64, 64]               0
           Conv2d-47           [-1, 64, 64, 64]          36,864
           Conv2d-48           [-1, 64, 64, 64]          36,864
      BatchNorm2d-49           [-1, 64, 64, 64]             128
      BatchNorm2d-50           [-1, 64, 64, 64]             128
             ReLU-51           [-1, 64, 64, 64]               0
             ReLU-52           [-1, 64, 64, 64]               0
       BasicBlock-53           [-1, 64, 64, 64]               0
       BasicBlock-54           [-1, 64, 64, 64]               0
           Conv2d-55          [-1, 128, 32, 32]          73,728
           Conv2d-56          [-1, 128, 32, 32]          73,728
      BatchNorm2d-57          [-1, 128, 32, 32]             256
      BatchNorm2d-58          [-1, 128, 32, 32]             256
             ReLU-59          [-1, 128, 32, 32]               0
             ReLU-60          [-1, 128, 32, 32]               0
           Conv2d-61          [-1, 128, 32, 32]         147,456
           Conv2d-62          [-1, 128, 32, 32]         147,456
      BatchNorm2d-63          [-1, 128, 32, 32]             256
      BatchNorm2d-64          [-1, 128, 32, 32]             256
           Conv2d-65          [-1, 128, 32, 32]           8,192
           Conv2d-66          [-1, 128, 32, 32]           8,192
      BatchNorm2d-67          [-1, 128, 32, 32]             256
      BatchNorm2d-68          [-1, 128, 32, 32]             256
             ReLU-69          [-1, 128, 32, 32]               0
             ReLU-70          [-1, 128, 32, 32]               0
       BasicBlock-71          [-1, 128, 32, 32]               0
       BasicBlock-72          [-1, 128, 32, 32]               0
           Conv2d-73          [-1, 128, 32, 32]         147,456
           Conv2d-74          [-1, 128, 32, 32]         147,456
      BatchNorm2d-75          [-1, 128, 32, 32]             256
      BatchNorm2d-76          [-1, 128, 32, 32]             256
             ReLU-77          [-1, 128, 32, 32]               0
             ReLU-78          [-1, 128, 32, 32]               0
           Conv2d-79          [-1, 128, 32, 32]         147,456
           Conv2d-80          [-1, 128, 32, 32]         147,456
      BatchNorm2d-81          [-1, 128, 32, 32]             256
      BatchNorm2d-82          [-1, 128, 32, 32]             256
             ReLU-83          [-1, 128, 32, 32]               0
             ReLU-84          [-1, 128, 32, 32]               0
       BasicBlock-85          [-1, 128, 32, 32]               0
       BasicBlock-86          [-1, 128, 32, 32]               0
           Conv2d-87          [-1, 128, 32, 32]         147,456
           Conv2d-88          [-1, 128, 32, 32]         147,456
      BatchNorm2d-89          [-1, 128, 32, 32]             256
      BatchNorm2d-90          [-1, 128, 32, 32]             256
             ReLU-91          [-1, 128, 32, 32]               0
             ReLU-92          [-1, 128, 32, 32]               0
           Conv2d-93          [-1, 128, 32, 32]         147,456
           Conv2d-94          [-1, 128, 32, 32]         147,456
      BatchNorm2d-95          [-1, 128, 32, 32]             256
      BatchNorm2d-96          [-1, 128, 32, 32]             256
             ReLU-97          [-1, 128, 32, 32]               0
             ReLU-98          [-1, 128, 32, 32]               0
       BasicBlock-99          [-1, 128, 32, 32]               0
      BasicBlock-100          [-1, 128, 32, 32]               0
          Conv2d-101          [-1, 128, 32, 32]         147,456
          Conv2d-102          [-1, 128, 32, 32]         147,456
     BatchNorm2d-103          [-1, 128, 32, 32]             256
     BatchNorm2d-104          [-1, 128, 32, 32]             256
            ReLU-105          [-1, 128, 32, 32]               0
            ReLU-106          [-1, 128, 32, 32]               0
          Conv2d-107          [-1, 128, 32, 32]         147,456
          Conv2d-108          [-1, 128, 32, 32]         147,456
     BatchNorm2d-109          [-1, 128, 32, 32]             256
     BatchNorm2d-110          [-1, 128, 32, 32]             256
            ReLU-111          [-1, 128, 32, 32]               0
            ReLU-112          [-1, 128, 32, 32]               0
      BasicBlock-113          [-1, 128, 32, 32]               0
      BasicBlock-114          [-1, 128, 32, 32]               0
          Conv2d-115          [-1, 256, 16, 16]         294,912
          Conv2d-116          [-1, 256, 16, 16]         294,912
     BatchNorm2d-117          [-1, 256, 16, 16]             512
     BatchNorm2d-118          [-1, 256, 16, 16]             512
            ReLU-119          [-1, 256, 16, 16]               0
            ReLU-120          [-1, 256, 16, 16]               0
          Conv2d-121          [-1, 256, 16, 16]         589,824
          Conv2d-122          [-1, 256, 16, 16]         589,824
     BatchNorm2d-123          [-1, 256, 16, 16]             512
     BatchNorm2d-124          [-1, 256, 16, 16]             512
          Conv2d-125          [-1, 256, 16, 16]          32,768
          Conv2d-126          [-1, 256, 16, 16]          32,768
     BatchNorm2d-127          [-1, 256, 16, 16]             512
     BatchNorm2d-128          [-1, 256, 16, 16]             512
            ReLU-129          [-1, 256, 16, 16]               0
            ReLU-130          [-1, 256, 16, 16]               0
      BasicBlock-131          [-1, 256, 16, 16]               0
      BasicBlock-132          [-1, 256, 16, 16]               0
          Conv2d-133          [-1, 256, 16, 16]         589,824
          Conv2d-134          [-1, 256, 16, 16]         589,824
     BatchNorm2d-135          [-1, 256, 16, 16]             512
     BatchNorm2d-136          [-1, 256, 16, 16]             512
            ReLU-137          [-1, 256, 16, 16]               0
            ReLU-138          [-1, 256, 16, 16]               0
          Conv2d-139          [-1, 256, 16, 16]         589,824
          Conv2d-140          [-1, 256, 16, 16]         589,824
     BatchNorm2d-141          [-1, 256, 16, 16]             512
     BatchNorm2d-142          [-1, 256, 16, 16]             512
            ReLU-143          [-1, 256, 16, 16]               0
            ReLU-144          [-1, 256, 16, 16]               0
      BasicBlock-145          [-1, 256, 16, 16]               0
      BasicBlock-146          [-1, 256, 16, 16]               0
          Conv2d-147          [-1, 256, 16, 16]         589,824
          Conv2d-148          [-1, 256, 16, 16]         589,824
     BatchNorm2d-149          [-1, 256, 16, 16]             512
     BatchNorm2d-150          [-1, 256, 16, 16]             512
            ReLU-151          [-1, 256, 16, 16]               0
            ReLU-152          [-1, 256, 16, 16]               0
          Conv2d-153          [-1, 256, 16, 16]         589,824
          Conv2d-154          [-1, 256, 16, 16]         589,824
     BatchNorm2d-155          [-1, 256, 16, 16]             512
     BatchNorm2d-156          [-1, 256, 16, 16]             512
            ReLU-157          [-1, 256, 16, 16]               0
            ReLU-158          [-1, 256, 16, 16]               0
      BasicBlock-159          [-1, 256, 16, 16]               0
      BasicBlock-160          [-1, 256, 16, 16]               0
          Conv2d-161          [-1, 256, 16, 16]         589,824
          Conv2d-162          [-1, 256, 16, 16]         589,824
     BatchNorm2d-163          [-1, 256, 16, 16]             512
     BatchNorm2d-164          [-1, 256, 16, 16]             512
            ReLU-165          [-1, 256, 16, 16]               0
            ReLU-166          [-1, 256, 16, 16]               0
          Conv2d-167          [-1, 256, 16, 16]         589,824
          Conv2d-168          [-1, 256, 16, 16]         589,824
     BatchNorm2d-169          [-1, 256, 16, 16]             512
     BatchNorm2d-170          [-1, 256, 16, 16]             512
            ReLU-171          [-1, 256, 16, 16]               0
            ReLU-172          [-1, 256, 16, 16]               0
      BasicBlock-173          [-1, 256, 16, 16]               0
      BasicBlock-174          [-1, 256, 16, 16]               0
          Conv2d-175          [-1, 256, 16, 16]         589,824
          Conv2d-176          [-1, 256, 16, 16]         589,824
     BatchNorm2d-177          [-1, 256, 16, 16]             512
     BatchNorm2d-178          [-1, 256, 16, 16]             512
            ReLU-179          [-1, 256, 16, 16]               0
            ReLU-180          [-1, 256, 16, 16]               0
          Conv2d-181          [-1, 256, 16, 16]         589,824
          Conv2d-182          [-1, 256, 16, 16]         589,824
     BatchNorm2d-183          [-1, 256, 16, 16]             512
     BatchNorm2d-184          [-1, 256, 16, 16]             512
            ReLU-185          [-1, 256, 16, 16]               0
            ReLU-186          [-1, 256, 16, 16]               0
      BasicBlock-187          [-1, 256, 16, 16]               0
      BasicBlock-188          [-1, 256, 16, 16]               0
          Conv2d-189          [-1, 256, 16, 16]         589,824
          Conv2d-190          [-1, 256, 16, 16]         589,824
     BatchNorm2d-191          [-1, 256, 16, 16]             512
     BatchNorm2d-192          [-1, 256, 16, 16]             512
            ReLU-193          [-1, 256, 16, 16]               0
            ReLU-194          [-1, 256, 16, 16]               0
          Conv2d-195          [-1, 256, 16, 16]         589,824
          Conv2d-196          [-1, 256, 16, 16]         589,824
     BatchNorm2d-197          [-1, 256, 16, 16]             512
     BatchNorm2d-198          [-1, 256, 16, 16]             512
            ReLU-199          [-1, 256, 16, 16]               0
            ReLU-200          [-1, 256, 16, 16]               0
      BasicBlock-201          [-1, 256, 16, 16]               0
      BasicBlock-202          [-1, 256, 16, 16]               0
          Conv2d-203            [-1, 512, 8, 8]       1,179,648
          Conv2d-204            [-1, 512, 8, 8]       1,179,648
     BatchNorm2d-205            [-1, 512, 8, 8]           1,024
     BatchNorm2d-206            [-1, 512, 8, 8]           1,024
            ReLU-207            [-1, 512, 8, 8]               0
            ReLU-208            [-1, 512, 8, 8]               0
          Conv2d-209            [-1, 512, 8, 8]       2,359,296
          Conv2d-210            [-1, 512, 8, 8]       2,359,296
     BatchNorm2d-211            [-1, 512, 8, 8]           1,024
     BatchNorm2d-212            [-1, 512, 8, 8]           1,024
          Conv2d-213            [-1, 512, 8, 8]         131,072
          Conv2d-214            [-1, 512, 8, 8]         131,072
     BatchNorm2d-215            [-1, 512, 8, 8]           1,024
     BatchNorm2d-216            [-1, 512, 8, 8]           1,024
            ReLU-217            [-1, 512, 8, 8]               0
            ReLU-218            [-1, 512, 8, 8]               0
      BasicBlock-219            [-1, 512, 8, 8]               0
      BasicBlock-220            [-1, 512, 8, 8]               0
          Conv2d-221            [-1, 512, 8, 8]       2,359,296
          Conv2d-222            [-1, 512, 8, 8]       2,359,296
     BatchNorm2d-223            [-1, 512, 8, 8]           1,024
     BatchNorm2d-224            [-1, 512, 8, 8]           1,024
            ReLU-225            [-1, 512, 8, 8]               0
            ReLU-226            [-1, 512, 8, 8]               0
          Conv2d-227            [-1, 512, 8, 8]       2,359,296
          Conv2d-228            [-1, 512, 8, 8]       2,359,296
     BatchNorm2d-229            [-1, 512, 8, 8]           1,024
     BatchNorm2d-230            [-1, 512, 8, 8]           1,024
            ReLU-231            [-1, 512, 8, 8]               0
            ReLU-232            [-1, 512, 8, 8]               0
      BasicBlock-233            [-1, 512, 8, 8]               0
      BasicBlock-234            [-1, 512, 8, 8]               0
          Conv2d-235            [-1, 512, 8, 8]       2,359,296
          Conv2d-236            [-1, 512, 8, 8]       2,359,296
     BatchNorm2d-237            [-1, 512, 8, 8]           1,024
     BatchNorm2d-238            [-1, 512, 8, 8]           1,024
            ReLU-239            [-1, 512, 8, 8]               0
            ReLU-240            [-1, 512, 8, 8]               0
          Conv2d-241            [-1, 512, 8, 8]       2,359,296
          Conv2d-242            [-1, 512, 8, 8]       2,359,296
     BatchNorm2d-243            [-1, 512, 8, 8]           1,024
     BatchNorm2d-244            [-1, 512, 8, 8]           1,024
            ReLU-245            [-1, 512, 8, 8]               0
            ReLU-246            [-1, 512, 8, 8]               0
      BasicBlock-247            [-1, 512, 8, 8]               0
      BasicBlock-248            [-1, 512, 8, 8]               0
          Conv2d-249            [-1, 512, 8, 8]         262,656
            ReLU-250            [-1, 512, 8, 8]               0
        Upsample-251          [-1, 512, 16, 16]               0
          Conv2d-252          [-1, 256, 16, 16]          65,792
            ReLU-253          [-1, 256, 16, 16]               0
          Conv2d-254          [-1, 512, 16, 16]       3,539,456
            ReLU-255          [-1, 512, 16, 16]               0
        Upsample-256          [-1, 512, 32, 32]               0
          Conv2d-257          [-1, 128, 32, 32]          16,512
            ReLU-258          [-1, 128, 32, 32]               0
          Conv2d-259          [-1, 256, 32, 32]       1,474,816
            ReLU-260          [-1, 256, 32, 32]               0
        Upsample-261          [-1, 256, 64, 64]               0
          Conv2d-262           [-1, 64, 64, 64]           4,160
            ReLU-263           [-1, 64, 64, 64]               0
          Conv2d-264          [-1, 256, 64, 64]         737,536
            ReLU-265          [-1, 256, 64, 64]               0
        Upsample-266        [-1, 256, 128, 128]               0
          Conv2d-267         [-1, 64, 128, 128]           4,160
            ReLU-268         [-1, 64, 128, 128]               0
          Conv2d-269        [-1, 128, 128, 128]         368,768
            ReLU-270        [-1, 128, 128, 128]               0
        Upsample-271        [-1, 128, 256, 256]               0
          Conv2d-272         [-1, 64, 256, 256]         110,656
            ReLU-273         [-1, 64, 256, 256]               0
          Conv2d-274          [-1, 2, 256, 256]             130
================================================================
Total params: 49,179,010
Trainable params: 49,179,010
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.25
Forward/backward pass size (MB): 631.00
Params size (MB): 187.60
Estimated Total Size (MB): 818.85
----------------------------------------------------------------
----------------------------------------------------------------
The number of train set: 1680
The number of valid set: 420
----------------------------------------------------------------
Epoch 0/199
----------
LR 0.003
train: bce: 0.086957, dice: 0.263636, loss: 0.210632
val: bce: 0.035958, dice: 0.243472, loss: 0.181218
Validation loss decreased (inf --> 0.181218).  Saving best model ...
0m 39s
Epoch 1/199
----------
LR 0.003
train: bce: 0.040213, dice: 0.251790, loss: 0.188317
val: bce: 0.034176, dice: 0.242111, loss: 0.179730
Validation loss decreased (0.181218 --> 0.179730).  Saving best model ...
0m 37s
Epoch 2/199
----------
LR 0.003
train: bce: 0.036993, dice: 0.251798, loss: 0.187356
val: bce: 0.036526, dice: 0.240981, loss: 0.179645
Validation loss decreased (0.179730 --> 0.179645).  Saving best model ...
0m 37s
Epoch 3/199
----------
LR 0.003
train: bce: 0.036270, dice: 0.251695, loss: 0.187068
val: bce: 0.032424, dice: 0.241975, loss: 0.179109
Validation loss decreased (0.179645 --> 0.179109).  Saving best model ...
0m 37s
Epoch 4/199
----------
LR 0.003
train: bce: 0.035752, dice: 0.251759, loss: 0.186957
val: bce: 0.034025, dice: 0.241075, loss: 0.178960
Validation loss decreased (0.179109 --> 0.178960).  Saving best model ...
0m 37s
Epoch 5/199
----------
LR 0.003
train: bce: 0.035493, dice: 0.251773, loss: 0.186889
val: bce: 0.030559, dice: 0.242589, loss: 0.178980
EarlyStopping counter: 1 out of 15
0m 37s
Epoch 6/199
----------
LR 0.003
train: bce: 0.036188, dice: 0.251743, loss: 0.187076
val: bce: 0.031760, dice: 0.241950, loss: 0.178893
Validation loss decreased (0.178960 --> 0.178893).  Saving best model ...
0m 37s
Epoch 7/199
----------
LR 0.003
train: bce: 0.033652, dice: 0.251664, loss: 0.186260
val: bce: 0.034093, dice: 0.241130, loss: 0.179019
EarlyStopping counter: 1 out of 15
0m 37s
Epoch 8/199
----------
LR 0.003
train: bce: 0.032750, dice: 0.251673, loss: 0.185996
val: bce: 0.034448, dice: 0.240780, loss: 0.178881
Validation loss decreased (0.178893 --> 0.178881).  Saving best model ...
0m 37s
Epoch 9/199
----------
LR 0.003
train: bce: 0.033240, dice: 0.251678, loss: 0.186146
val: bce: 0.031423, dice: 0.241479, loss: 0.178462
Validation loss decreased (0.178881 --> 0.178462).  Saving best model ...
0m 38s
Epoch 10/199
----------
LR 0.003
train: bce: 0.031811, dice: 0.251566, loss: 0.185640
val: bce: 0.027999, dice: 0.242783, loss: 0.178348
Validation loss decreased (0.178462 --> 0.178348).  Saving best model ...
0m 38s
Epoch 11/199
----------
LR 0.003
train: bce: 0.033447, dice: 0.251732, loss: 0.186247
val: bce: 0.027496, dice: 0.244373, loss: 0.179310
EarlyStopping counter: 1 out of 15
0m 37s
Epoch 12/199
----------
LR 0.003
train: bce: 0.034016, dice: 0.251696, loss: 0.186392
val: bce: 0.027820, dice: 0.244162, loss: 0.179260
EarlyStopping counter: 2 out of 15
0m 37s
Epoch 13/199
----------
LR 0.003
train: bce: 0.031136, dice: 0.251612, loss: 0.185469
val: bce: 0.034225, dice: 0.240728, loss: 0.178777
EarlyStopping counter: 3 out of 15
0m 37s
Epoch 14/199
----------
LR 0.003
train: bce: 0.038861, dice: 0.252083, loss: 0.188116
val: bce: 0.029738, dice: 0.243358, loss: 0.179272
EarlyStopping counter: 4 out of 15
0m 37s
Epoch 15/199
----------
LR 0.003
train: bce: 0.032993, dice: 0.251978, loss: 0.186283
val: bce: 0.032280, dice: 0.241146, loss: 0.178486
EarlyStopping counter: 5 out of 15
0m 37s
Epoch 16/199
----------
LR 0.00030000000000000003
train: bce: 0.031931, dice: 0.251563, loss: 0.185674
val: bce: 0.030236, dice: 0.241572, loss: 0.178171
Validation loss decreased (0.178348 --> 0.178171).  Saving best model ...
0m 37s
Epoch 17/199
----------
LR 0.00030000000000000003
train: bce: 0.031316, dice: 0.251544, loss: 0.185476
val: bce: 0.030672, dice: 0.241584, loss: 0.178310
EarlyStopping counter: 1 out of 15
0m 37s
Epoch 18/199
----------
LR 0.00030000000000000003
train: bce: 0.030904, dice: 0.251519, loss: 0.185335
val: bce: 0.029658, dice: 0.241632, loss: 0.178040
Validation loss decreased (0.178171 --> 0.178040).  Saving best model ...
0m 37s
Epoch 19/199
----------
LR 0.00030000000000000003
train: bce: 0.030676, dice: 0.251528, loss: 0.185272
val: bce: 0.029302, dice: 0.241849, loss: 0.178085
EarlyStopping counter: 1 out of 15
0m 38s
Epoch 20/199
----------
LR 0.00030000000000000003
train: bce: 0.030371, dice: 0.251495, loss: 0.185158
val: bce: 0.029147, dice: 0.242009, loss: 0.178150
EarlyStopping counter: 2 out of 15
0m 38s
Epoch 21/199
----------
LR 0.00030000000000000003
train: bce: 0.030415, dice: 0.251495, loss: 0.185171
val: bce: 0.028071, dice: 0.242711, loss: 0.178319
EarlyStopping counter: 3 out of 15
0m 37s
Epoch 22/199
----------
LR 0.00030000000000000003
train: bce: 0.030140, dice: 0.251571, loss: 0.185141
val: bce: 0.029379, dice: 0.241636, loss: 0.177959
Validation loss decreased (0.178040 --> 0.177959).  Saving best model ...
0m 38s
Epoch 23/199
----------
LR 0.00030000000000000003
train: bce: 0.029851, dice: 0.251477, loss: 0.184989
val: bce: 0.028063, dice: 0.241702, loss: 0.177610
Validation loss decreased (0.177959 --> 0.177610).  Saving best model ...
0m 38s
Epoch 24/199
----------
LR 0.00030000000000000003
train: bce: 0.029924, dice: 0.251518, loss: 0.185039
val: bce: 0.028318, dice: 0.242010, loss: 0.177903
EarlyStopping counter: 1 out of 15
0m 37s
Epoch 25/199
----------
LR 0.00030000000000000003
train: bce: 0.029774, dice: 0.251506, loss: 0.184986
val: bce: 0.029628, dice: 0.241287, loss: 0.177789
EarlyStopping counter: 2 out of 15
0m 37s
Epoch 26/199
----------
LR 0.00030000000000000003
train: bce: 0.029097, dice: 0.251492, loss: 0.184773
val: bce: 0.029728, dice: 0.241166, loss: 0.177735
EarlyStopping counter: 3 out of 15
0m 37s
Epoch 27/199
----------
LR 0.00030000000000000003
train: bce: 0.029516, dice: 0.251468, loss: 0.184882
val: bce: 0.029093, dice: 0.241650, loss: 0.177883
EarlyStopping counter: 4 out of 15
0m 37s
Epoch 28/199
----------
LR 0.00030000000000000003
train: bce: 0.029340, dice: 0.251453, loss: 0.184819
val: bce: 0.028929, dice: 0.241390, loss: 0.177652
EarlyStopping counter: 5 out of 15
0m 36s
Epoch 29/199
----------
LR 3.0000000000000004e-05
train: bce: 0.028993, dice: 0.251388, loss: 0.184669
val: bce: 0.029110, dice: 0.241453, loss: 0.177750
EarlyStopping counter: 6 out of 15
0m 37s
Epoch 30/199
----------
LR 3.0000000000000004e-05
train: bce: 0.028730, dice: 0.251546, loss: 0.184701
val: bce: 0.028412, dice: 0.241573, loss: 0.177625
EarlyStopping counter: 7 out of 15
0m 37s
Epoch 31/199
----------
LR 3.0000000000000004e-05
train: bce: 0.029183, dice: 0.251407, loss: 0.184740
val: bce: 0.029055, dice: 0.241348, loss: 0.177660
EarlyStopping counter: 8 out of 15
0m 37s
Epoch 32/199
----------
LR 3.0000000000000004e-05
train: bce: 0.028720, dice: 0.251423, loss: 0.184612
val: bce: 0.028746, dice: 0.241572, loss: 0.177725
EarlyStopping counter: 9 out of 15
0m 36s
Epoch 33/199
----------
LR 3.0000000000000004e-05
train: bce: 0.028757, dice: 0.251382, loss: 0.184595
val: bce: 0.028198, dice: 0.241751, loss: 0.177685
EarlyStopping counter: 10 out of 15
0m 37s
Epoch 34/199
----------
LR 3.0000000000000005e-06
train: bce: 0.028527, dice: 0.251496, loss: 0.184605
val: bce: 0.028092, dice: 0.241835, loss: 0.177712
EarlyStopping counter: 11 out of 15
0m 37s
Epoch 35/199
----------
LR 3.0000000000000005e-06
train: bce: 0.028879, dice: 0.251499, loss: 0.184713
val: bce: 0.028820, dice: 0.241590, loss: 0.177759
EarlyStopping counter: 12 out of 15
0m 37s
Epoch 36/199
----------
LR 3.0000000000000005e-06
train: bce: 0.028833, dice: 0.251434, loss: 0.184654
val: bce: 0.028155, dice: 0.241749, loss: 0.177671
EarlyStopping counter: 13 out of 15
0m 37s
Epoch 37/199
----------
LR 3.0000000000000005e-06
train: bce: 0.028565, dice: 0.251410, loss: 0.184557
val: bce: 0.028426, dice: 0.241742, loss: 0.177747
EarlyStopping counter: 14 out of 15
0m 37s
Epoch 38/199
----------
LR 3.0000000000000005e-06
train: bce: 0.028534, dice: 0.251433, loss: 0.184564
val: bce: 0.027867, dice: 0.241838, loss: 0.177646
EarlyStopping counter: 15 out of 15
0m 38s
Early stopping
Best val loss: 0.177610
(torch) chan@chan-desktop:~/Desktop/DLD1$ python test.py --epochs 200 --lr 0.003 --model resnetunet --freeze True --metric-dataset-type undersample
----------------------------------------------------------------
The number of test set: 3205
----------------------------------------------------------------
----------
The Evaluation Starts ...
----------
The total samples: 3205
The average dice score is 0.9602361917495728.
The number of tumor samples: 239
The average tumor dice score is 0.5011301636695862.
The number of non-tumor samples: 2966
The average non tumor dice score is 0.9972312450408936.
0m 58s

----------------------------------------------------------------
Fine Tuning
----------------------------------------------------------------
(torch) chan@chan-desktop:~/Desktop/DLD1$ python train.py --epochs 50 --lr 0.00003 --model resnetunet --freeze False --load True --dataset-type undersample --load-epoch 200
----------------------------------------------------------------
----------------------------------------------------------------
The number of train set: 1680
The number of valid set: 420
----------------------------------------------------------------
Epoch 0/49
----------
LR 3e-05
train: bce: 0.028983, dice: 0.246302, loss: 0.181106
val: bce: 0.031181, dice: 0.262326, loss: 0.192983
Validation loss decreased (inf --> 0.192983).  Saving best model ...
0m 38s
Epoch 1/49
----------
LR 3e-05
train: bce: 0.028970, dice: 0.246274, loss: 0.181083
val: bce: 0.031361, dice: 0.262270, loss: 0.192998
EarlyStopping counter: 1 out of 15
0m 36s
Epoch 2/49
----------
LR 3e-05
train: bce: 0.029078, dice: 0.246303, loss: 0.181136
val: bce: 0.031023, dice: 0.262248, loss: 0.192881
Validation loss decreased (0.192983 --> 0.192881).  Saving best model ...
0m 36s
Epoch 3/49
----------
LR 3e-05
train: bce: 0.028998, dice: 0.246189, loss: 0.181032
val: bce: 0.030646, dice: 0.262446, loss: 0.192906
EarlyStopping counter: 1 out of 15
0m 36s
Epoch 4/49
----------
LR 3e-05
train: bce: 0.028839, dice: 0.246263, loss: 0.181036
val: bce: 0.031199, dice: 0.262248, loss: 0.192933
EarlyStopping counter: 2 out of 15
0m 36s
Epoch 5/49
----------
LR 3e-05
train: bce: 0.028558, dice: 0.246187, loss: 0.180898
val: bce: 0.031230, dice: 0.262440, loss: 0.193077
EarlyStopping counter: 3 out of 15
0m 37s
Epoch 6/49
----------
LR 3e-05
train: bce: 0.028446, dice: 0.246273, loss: 0.180925
val: bce: 0.031208, dice: 0.262315, loss: 0.192983
EarlyStopping counter: 4 out of 15
0m 37s
Epoch 7/49
----------
LR 3e-05
train: bce: 0.028885, dice: 0.246237, loss: 0.181031
val: bce: 0.030669, dice: 0.262359, loss: 0.192852
Validation loss decreased (0.192881 --> 0.192852).  Saving best model ...
0m 37s
Epoch 8/49
----------
LR 3e-05
train: bce: 0.028887, dice: 0.246231, loss: 0.181028
val: bce: 0.030706, dice: 0.262349, loss: 0.192856
EarlyStopping counter: 1 out of 15
0m 37s
Epoch 9/49
----------
LR 3e-05
train: bce: 0.028450, dice: 0.246265, loss: 0.180921
val: bce: 0.032130, dice: 0.261997, loss: 0.193037
EarlyStopping counter: 2 out of 15
0m 37s
Epoch 10/49
----------
LR 3e-05
train: bce: 0.028704, dice: 0.246211, loss: 0.180959
val: bce: 0.030708, dice: 0.262374, loss: 0.192874
EarlyStopping counter: 3 out of 15
0m 37s
Epoch 11/49
----------
LR 3e-05
train: bce: 0.028593, dice: 0.246281, loss: 0.180975
val: bce: 0.030483, dice: 0.262282, loss: 0.192742
Validation loss decreased (0.192852 --> 0.192742).  Saving best model ...
0m 39s
Epoch 12/49
----------
LR 3e-05
train: bce: 0.028356, dice: 0.246231, loss: 0.180869
val: bce: 0.030107, dice: 0.262474, loss: 0.192764
EarlyStopping counter: 1 out of 15
0m 39s
Epoch 13/49
----------
LR 3e-05
train: bce: 0.028462, dice: 0.246238, loss: 0.180905
val: bce: 0.030447, dice: 0.262367, loss: 0.192791
EarlyStopping counter: 2 out of 15
0m 37s
Epoch 14/49
----------
LR 3e-05
train: bce: 0.028625, dice: 0.246220, loss: 0.180942
val: bce: 0.029742, dice: 0.262461, loss: 0.192645
Validation loss decreased (0.192742 --> 0.192645).  Saving best model ...
0m 38s
Epoch 15/49
----------
LR 3e-05
train: bce: 0.028317, dice: 0.246222, loss: 0.180851
val: bce: 0.030658, dice: 0.262584, loss: 0.193006
EarlyStopping counter: 1 out of 15
0m 37s
Epoch 16/49
----------
LR 3e-05
train: bce: 0.028415, dice: 0.246269, loss: 0.180913
val: bce: 0.030573, dice: 0.262453, loss: 0.192889
EarlyStopping counter: 2 out of 15
0m 37s
Epoch 17/49
----------
LR 3e-05
train: bce: 0.028623, dice: 0.246244, loss: 0.180958
val: bce: 0.030468, dice: 0.262519, loss: 0.192904
EarlyStopping counter: 3 out of 15
0m 37s
Epoch 18/49
----------
LR 3e-05
train: bce: 0.028317, dice: 0.246262, loss: 0.180879
val: bce: 0.030486, dice: 0.262245, loss: 0.192717
EarlyStopping counter: 4 out of 15
0m 37s
Epoch 19/49
----------
LR 3e-05
train: bce: 0.028593, dice: 0.246205, loss: 0.180921
val: bce: 0.030363, dice: 0.262278, loss: 0.192704
EarlyStopping counter: 5 out of 15
0m 36s
Epoch 20/49
----------
LR 3e-06
train: bce: 0.028029, dice: 0.246122, loss: 0.180694
val: bce: 0.030085, dice: 0.262416, loss: 0.192717
EarlyStopping counter: 6 out of 15
0m 37s
Epoch 21/49
----------
LR 3e-06
train: bce: 0.028196, dice: 0.246166, loss: 0.180775
val: bce: 0.030567, dice: 0.262340, loss: 0.192808
EarlyStopping counter: 7 out of 15
0m 37s
Epoch 22/49
----------
LR 3e-06
train: bce: 0.028074, dice: 0.246334, loss: 0.180856
val: bce: 0.030160, dice: 0.262216, loss: 0.192599
Validation loss decreased (0.192645 --> 0.192599).  Saving best model ...
0m 37s
Epoch 23/49
----------
LR 3e-06
train: bce: 0.028738, dice: 0.246254, loss: 0.180999
val: bce: 0.030371, dice: 0.262221, loss: 0.192666
EarlyStopping counter: 1 out of 15
0m 37s
Epoch 24/49
----------
LR 3e-06
train: bce: 0.028392, dice: 0.246220, loss: 0.180871
val: bce: 0.030615, dice: 0.262321, loss: 0.192809
EarlyStopping counter: 2 out of 15
0m 38s
Epoch 25/49
----------
LR 3e-06
train: bce: 0.028124, dice: 0.246154, loss: 0.180745
val: bce: 0.030016, dice: 0.262411, loss: 0.192692
EarlyStopping counter: 3 out of 15
0m 38s
Epoch 26/49
----------
LR 3e-06
train: bce: 0.028134, dice: 0.246304, loss: 0.180853
val: bce: 0.030285, dice: 0.262289, loss: 0.192688
EarlyStopping counter: 4 out of 15
0m 38s
Epoch 27/49
----------
LR 3e-06
train: bce: 0.028161, dice: 0.246236, loss: 0.180813
val: bce: 0.030520, dice: 0.262233, loss: 0.192719
EarlyStopping counter: 5 out of 15
0m 36s
Epoch 28/49
----------
LR 3.0000000000000004e-07
train: bce: 0.028269, dice: 0.246243, loss: 0.180851
val: bce: 0.030789, dice: 0.262133, loss: 0.192729
EarlyStopping counter: 6 out of 15
0m 38s
Epoch 29/49
----------
LR 3.0000000000000004e-07
train: bce: 0.028641, dice: 0.246218, loss: 0.180945
val: bce: 0.030628, dice: 0.262281, loss: 0.192785
EarlyStopping counter: 7 out of 15
0m 38s
Epoch 30/49
----------
LR 3.0000000000000004e-07
train: bce: 0.028169, dice: 0.246146, loss: 0.180753
val: bce: 0.030156, dice: 0.262367, loss: 0.192704
EarlyStopping counter: 8 out of 15
0m 38s
Epoch 31/49
----------
LR 3.0000000000000004e-07
train: bce: 0.028243, dice: 0.246158, loss: 0.180784
val: bce: 0.030499, dice: 0.262333, loss: 0.192783
EarlyStopping counter: 9 out of 15
0m 38s
Epoch 32/49
----------
LR 3.0000000000000004e-07
train: bce: 0.028571, dice: 0.246195, loss: 0.180908
val: bce: 0.030504, dice: 0.262248, loss: 0.192725
EarlyStopping counter: 10 out of 15
0m 38s
Epoch 33/49
----------
LR 3.0000000000000004e-08
train: bce: 0.028207, dice: 0.246224, loss: 0.180819
val: bce: 0.029979, dice: 0.262335, loss: 0.192629
EarlyStopping counter: 11 out of 15
0m 39s
Epoch 34/49
----------
LR 3.0000000000000004e-08
train: bce: 0.028247, dice: 0.246239, loss: 0.180841
val: bce: 0.030351, dice: 0.262504, loss: 0.192858
EarlyStopping counter: 12 out of 15
0m 38s
Epoch 35/49
----------
LR 3.0000000000000004e-08
train: bce: 0.028006, dice: 0.246187, loss: 0.180733
val: bce: 0.030435, dice: 0.262239, loss: 0.192698
EarlyStopping counter: 13 out of 15
0m 38s
Epoch 36/49
----------
LR 3.0000000000000004e-08
train: bce: 0.028370, dice: 0.246242, loss: 0.180881
val: bce: 0.030153, dice: 0.262162, loss: 0.192560
Validation loss decreased (0.192599 --> 0.192560).  Saving best model ...
0m 38s
Epoch 37/49
----------
LR 3.0000000000000004e-08
train: bce: 0.028219, dice: 0.246201, loss: 0.180806
val: bce: 0.030253, dice: 0.262274, loss: 0.192668
EarlyStopping counter: 1 out of 15
0m 37s
Epoch 38/49
----------
LR 3.0000000000000004e-08
train: bce: 0.028190, dice: 0.246160, loss: 0.180769
val: bce: 0.030663, dice: 0.262636, loss: 0.193044
EarlyStopping counter: 2 out of 15
0m 37s
Epoch 39/49
----------
LR 3.0000000000000004e-08
train: bce: 0.028300, dice: 0.246205, loss: 0.180833
val: bce: 0.030791, dice: 0.262297, loss: 0.192845
EarlyStopping counter: 3 out of 15
0m 37s
Epoch 40/49
----------
LR 3.0000000000000004e-08
train: bce: 0.028139, dice: 0.246180, loss: 0.180768
val: bce: 0.030010, dice: 0.262083, loss: 0.192461
Validation loss decreased (0.192560 --> 0.192461).  Saving best model ...
0m 38s
Epoch 41/49
----------
LR 3.0000000000000004e-08
train: bce: 0.028255, dice: 0.246178, loss: 0.180801
val: bce: 0.030480, dice: 0.262472, loss: 0.192874
EarlyStopping counter: 1 out of 15
0m 38s
Epoch 42/49
----------
LR 3.0000000000000004e-08
train: bce: 0.028283, dice: 0.246272, loss: 0.180875
val: bce: 0.030714, dice: 0.262560, loss: 0.193006
EarlyStopping counter: 2 out of 15
0m 37s
Epoch 43/49
----------
LR 3.0000000000000004e-08
train: bce: 0.028290, dice: 0.246179, loss: 0.180813
val: bce: 0.030568, dice: 0.262381, loss: 0.192837
EarlyStopping counter: 3 out of 15
0m 39s
Epoch 44/49
----------
LR 3.0000000000000004e-08
train: bce: 0.028528, dice: 0.246236, loss: 0.180924
val: bce: 0.030730, dice: 0.262320, loss: 0.192843
EarlyStopping counter: 4 out of 15
0m 38s
Epoch 45/49
----------
LR 3.0000000000000004e-08
train: bce: 0.028159, dice: 0.246248, loss: 0.180821
val: bce: 0.030946, dice: 0.262221, loss: 0.192839
EarlyStopping counter: 5 out of 15
0m 37s
Epoch 46/49
----------
LR 3.0000000000000004e-09
train: bce: 0.028354, dice: 0.246272, loss: 0.180897
val: bce: 0.030338, dice: 0.262211, loss: 0.192649
EarlyStopping counter: 6 out of 15
0m 37s
Epoch 47/49
----------
LR 3.0000000000000004e-09
train: bce: 0.028414, dice: 0.246199, loss: 0.180864
val: bce: 0.030383, dice: 0.262441, loss: 0.192823
EarlyStopping counter: 7 out of 15
0m 36s
Epoch 48/49
----------
LR 3.0000000000000004e-09
train: bce: 0.027951, dice: 0.246202, loss: 0.180726
val: bce: 0.030720, dice: 0.262280, loss: 0.192812
EarlyStopping counter: 8 out of 15
0m 38s
Epoch 49/49
----------
LR 3.0000000000000004e-09
train: bce: 0.028353, dice: 0.246253, loss: 0.180883
val: bce: 0.030435, dice: 0.262341, loss: 0.192769
EarlyStopping counter: 9 out of 15
0m 37s
Best val loss: 0.192461

(torch) chan@chan-desktop:~/Desktop/DLD1$ python test.py --epochs 50 --lr 0.00003 --model resnetunet --freeze False --metric-dataset-type undersample
----------------------------------------------------------------
The number of test set: 3205
----------------------------------------------------------------
----------
The Evaluation Starts ...
----------
The total samples: 3205
The average dice score is 0.960395097732544.
The number of tumor samples: 239
The average tumor dice score is 0.5012636780738831.
The number of non-tumor samples: 2966
The average non tumor dice score is 0.9973920583724976.
0m 57s