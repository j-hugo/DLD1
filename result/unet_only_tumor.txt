(torch) chan@chan-desktop:~/Desktop/DLD1$ python train.py --model unet --dataset-type only_tumor
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 256, 256]             640
       BatchNorm2d-2         [-1, 64, 256, 256]             128
              ReLU-3         [-1, 64, 256, 256]               0
            Conv2d-4         [-1, 64, 256, 256]          36,928
       BatchNorm2d-5         [-1, 64, 256, 256]             128
              ReLU-6         [-1, 64, 256, 256]               0
         MaxPool2d-7         [-1, 64, 128, 128]               0
            Conv2d-8        [-1, 128, 128, 128]          73,856
       BatchNorm2d-9        [-1, 128, 128, 128]             256
             ReLU-10        [-1, 128, 128, 128]               0
           Conv2d-11        [-1, 128, 128, 128]         147,584
      BatchNorm2d-12        [-1, 128, 128, 128]             256
             ReLU-13        [-1, 128, 128, 128]               0
        MaxPool2d-14          [-1, 128, 64, 64]               0
           Conv2d-15          [-1, 256, 64, 64]         295,168
      BatchNorm2d-16          [-1, 256, 64, 64]             512
             ReLU-17          [-1, 256, 64, 64]               0
           Conv2d-18          [-1, 256, 64, 64]         590,080
      BatchNorm2d-19          [-1, 256, 64, 64]             512
             ReLU-20          [-1, 256, 64, 64]               0
        MaxPool2d-21          [-1, 256, 32, 32]               0
           Conv2d-22          [-1, 512, 32, 32]       1,180,160
      BatchNorm2d-23          [-1, 512, 32, 32]           1,024
             ReLU-24          [-1, 512, 32, 32]               0
           Conv2d-25          [-1, 512, 32, 32]       2,359,808
      BatchNorm2d-26          [-1, 512, 32, 32]           1,024
             ReLU-27          [-1, 512, 32, 32]               0
        MaxPool2d-28          [-1, 512, 16, 16]               0
           Conv2d-29         [-1, 1024, 16, 16]       4,719,616
      BatchNorm2d-30         [-1, 1024, 16, 16]           2,048
             ReLU-31         [-1, 1024, 16, 16]               0
           Conv2d-32         [-1, 1024, 16, 16]       9,438,208
      BatchNorm2d-33         [-1, 1024, 16, 16]           2,048
             ReLU-34         [-1, 1024, 16, 16]               0
         Upsample-35         [-1, 1024, 32, 32]               0
           Conv2d-36          [-1, 512, 32, 32]       7,078,400
             ReLU-37          [-1, 512, 32, 32]               0
           Conv2d-38          [-1, 512, 32, 32]       2,359,808
             ReLU-39          [-1, 512, 32, 32]               0
         Upsample-40          [-1, 512, 64, 64]               0
           Conv2d-41          [-1, 256, 64, 64]       1,769,728
             ReLU-42          [-1, 256, 64, 64]               0
           Conv2d-43          [-1, 256, 64, 64]         590,080
             ReLU-44          [-1, 256, 64, 64]               0
         Upsample-45        [-1, 256, 128, 128]               0
           Conv2d-46        [-1, 128, 128, 128]         442,496
             ReLU-47        [-1, 128, 128, 128]               0
           Conv2d-48        [-1, 128, 128, 128]         147,584
             ReLU-49        [-1, 128, 128, 128]               0
         Upsample-50        [-1, 128, 256, 256]               0
           Conv2d-51         [-1, 64, 256, 256]         110,656
             ReLU-52         [-1, 64, 256, 256]               0
           Conv2d-53         [-1, 64, 256, 256]          36,928
             ReLU-54         [-1, 64, 256, 256]               0
           Conv2d-55          [-1, 1, 256, 256]              65
================================================================
Total params: 31,385,729
Trainable params: 31,385,729
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.25
Forward/backward pass size (MB): 747.50
Params size (MB): 119.73
Estimated Total Size (MB): 867.48
----------------------------------------------------------------
----------------------------------------------------------------
The number of train set: 1032
The number of valid set: 120
----------------------------------------------------------------
Epoch 0/299
----------
LR 0.003
train: bce: 0.150015, dice: 0.983940, loss: 1.208963
val: bce: 0.037824, dice: 0.968150, loss: 1.024886
Validation loss decreased (inf --> 1.024886).  Saving best model ...
0m 42s
Epoch 1/299
----------
LR 0.003
train: bce: 0.035724, dice: 0.957707, loss: 1.011293
val: bce: 0.033723, dice: 0.958080, loss: 1.008664
Validation loss decreased (1.024886 --> 1.008664).  Saving best model ...
0m 41s
Epoch 2/299
----------
LR 0.003
train: bce: 0.036938, dice: 0.947274, loss: 1.002681
val: bce: 0.042199, dice: 0.954187, loss: 1.017485
EarlyStopping counter: 1 out of 15
0m 41s
Epoch 3/299
----------
LR 0.003
train: bce: 0.036517, dice: 0.936747, loss: 0.991523
val: bce: 0.031566, dice: 0.926982, loss: 0.974332
Validation loss decreased (1.008664 --> 0.974332).  Saving best model ...
0m 42s
Epoch 4/299
----------
LR 0.003
train: bce: 0.035417, dice: 0.891162, loss: 0.944288
val: bce: 0.029061, dice: 0.873253, loss: 0.916844
Validation loss decreased (0.974332 --> 0.916844).  Saving best model ...
0m 41s
Epoch 5/299
----------
LR 0.003
train: bce: 0.031375, dice: 0.832543, loss: 0.879605
val: bce: 0.038392, dice: 0.848363, loss: 0.905951
Validation loss decreased (0.916844 --> 0.905951).  Saving best model ...
0m 42s
Epoch 6/299
----------
LR 0.003
train: bce: 0.031304, dice: 0.753560, loss: 0.800517
val: bce: 0.033181, dice: 0.729709, loss: 0.779480
Validation loss decreased (0.905951 --> 0.779480).  Saving best model ...
0m 41s
Epoch 7/299
----------
LR 0.003
train: bce: 0.036443, dice: 0.666536, loss: 0.721200
val: bce: 0.039191, dice: 0.695639, loss: 0.754425
Validation loss decreased (0.779480 --> 0.754425).  Saving best model ...
0m 42s
Epoch 8/299
----------
LR 0.003
train: bce: 0.034992, dice: 0.604499, loss: 0.656987
val: bce: 0.083179, dice: 0.950685, loss: 1.075452
EarlyStopping counter: 1 out of 15
0m 41s
Epoch 9/299
----------
LR 0.003
train: bce: 0.033467, dice: 0.586253, loss: 0.636455
val: bce: 0.026493, dice: 0.621270, loss: 0.661009
Validation loss decreased (0.754425 --> 0.661009).  Saving best model ...
0m 42s
Epoch 10/299
----------
LR 0.003
train: bce: 0.029547, dice: 0.542913, loss: 0.587234
val: bce: 0.027990, dice: 0.627465, loss: 0.669449
EarlyStopping counter: 1 out of 15
0m 41s
Epoch 11/299
----------
LR 0.003
train: bce: 0.028369, dice: 0.514768, loss: 0.557321
val: bce: 0.027462, dice: 0.660660, loss: 0.701852
EarlyStopping counter: 2 out of 15
0m 42s
Epoch 12/299
----------
LR 0.003
train: bce: 0.025110, dice: 0.480688, loss: 0.518353
val: bce: 0.029994, dice: 0.576277, loss: 0.621268
Validation loss decreased (0.661009 --> 0.621268).  Saving best model ...
0m 42s
Epoch 13/299
----------
LR 0.003
train: bce: 0.027871, dice: 0.487133, loss: 0.528939
val: bce: 0.050978, dice: 0.624808, loss: 0.701275
EarlyStopping counter: 1 out of 15
0m 41s
Epoch 14/299
----------
LR 0.003
train: bce: 0.025986, dice: 0.470460, loss: 0.509440
val: bce: 0.043728, dice: 0.610870, loss: 0.676461
EarlyStopping counter: 2 out of 15
0m 41s
Epoch 15/299
----------
LR 0.003
train: bce: 0.023966, dice: 0.451535, loss: 0.487484
val: bce: 0.023294, dice: 0.511799, loss: 0.546739
Validation loss decreased (0.621268 --> 0.546739).  Saving best model ...
0m 41s
Epoch 16/299
----------
LR 0.003
train: bce: 0.022397, dice: 0.429778, loss: 0.463373
val: bce: 0.021459, dice: 0.534963, loss: 0.567152
EarlyStopping counter: 1 out of 15
0m 41s
Epoch 17/299
----------
LR 0.003
train: bce: 0.024565, dice: 0.443025, loss: 0.479872
val: bce: 0.034522, dice: 0.552309, loss: 0.604092
EarlyStopping counter: 2 out of 15
0m 41s
Epoch 18/299
----------
LR 0.003
train: bce: 0.021111, dice: 0.408475, loss: 0.440143
val: bce: 0.038517, dice: 0.633803, loss: 0.691579
EarlyStopping counter: 3 out of 15
0m 41s
Epoch 19/299
----------
LR 0.003
train: bce: 0.021596, dice: 0.416502, loss: 0.448896
val: bce: 0.038296, dice: 0.734901, loss: 0.792346
EarlyStopping counter: 4 out of 15
0m 41s
Epoch 20/299
----------
LR 0.003
train: bce: 0.022220, dice: 0.414003, loss: 0.447333
val: bce: 0.026441, dice: 0.539731, loss: 0.579392
EarlyStopping counter: 5 out of 15
0m 41s
Epoch 21/299
----------
LR 0.003
train: bce: 0.019914, dice: 0.386692, loss: 0.416563
val: bce: 0.031557, dice: 0.586772, loss: 0.634107
EarlyStopping counter: 6 out of 15
0m 41s
Epoch 22/299
----------
LR 0.0015
train: bce: 0.017653, dice: 0.364135, loss: 0.390614
val: bce: 0.020691, dice: 0.471797, loss: 0.502833
Validation loss decreased (0.546739 --> 0.502833).  Saving best model ...
0m 41s
Epoch 23/299
----------
LR 0.0015
train: bce: 0.016417, dice: 0.336874, loss: 0.361500
val: bce: 0.017803, dice: 0.442831, loss: 0.469536
Validation loss decreased (0.502833 --> 0.469536).  Saving best model ...
0m 41s
Epoch 24/299
----------
LR 0.0015
train: bce: 0.015371, dice: 0.328720, loss: 0.351777
val: bce: 0.020277, dice: 0.489082, loss: 0.519497
EarlyStopping counter: 1 out of 15
0m 40s
Epoch 25/299
----------
LR 0.0015
train: bce: 0.015595, dice: 0.324463, loss: 0.347855
val: bce: 0.017082, dice: 0.433928, loss: 0.459551
Validation loss decreased (0.469536 --> 0.459551).  Saving best model ...
0m 41s
Epoch 26/299
----------
LR 0.0015
train: bce: 0.014867, dice: 0.317402, loss: 0.339703
val: bce: 0.019129, dice: 0.437959, loss: 0.466653
EarlyStopping counter: 1 out of 15
0m 40s
Epoch 27/299
----------
LR 0.0015
train: bce: 0.015798, dice: 0.326529, loss: 0.350226
val: bce: 0.019494, dice: 0.427560, loss: 0.456801
Validation loss decreased (0.459551 --> 0.456801).  Saving best model ...
0m 41s
Epoch 28/299
----------
LR 0.0015
train: bce: 0.015754, dice: 0.319389, loss: 0.343021
val: bce: 0.016610, dice: 0.429642, loss: 0.454557
Validation loss decreased (0.456801 --> 0.454557).  Saving best model ...
0m 41s
Epoch 29/299
----------
LR 0.0015
train: bce: 0.015909, dice: 0.316878, loss: 0.340742
val: bce: 0.018809, dice: 0.418655, loss: 0.446868
Validation loss decreased (0.454557 --> 0.446868).  Saving best model ...
0m 40s

----------------------------------------------------------------
The number of test set: 1189
----------------------------------------------------------------
----------
The Evaluation Starts ...
----------
The total samples: 1189
The average dice score is tensor([[0.5637]], device='cuda:0').
The number of tumor samples: 142
The average dice score of the slices which have tumor is tensor([[0.4597]], device='cuda:0').
The number of non-tumor samples: 1047
The average dice score of the slices which have non-tumor is tensor([[0.5778]], device='cuda:0').
0m 28s
