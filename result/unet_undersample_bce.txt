torch) chan@chan-desktop:~/Desktop/DLD1$ python train.py --model unet --dataset-type undersample
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 256, 256]             640
       BatchNorm2d-2         [-1, 64, 256, 256]             128
              ReLU-3         [-1, 64, 256, 256]               0
            Conv2d-4         [-1, 64, 256, 256]          36,928
       BatchNorm2d-5         [-1, 64, 256, 256]             128
              ReLU-6         [-1, 64, 256, 256]               0
         MaxPool2d-7         [-1, 64, 128, 128]               0
            Conv2d-8        [-1, 128, 128, 128]          73,856
       BatchNorm2d-9        [-1, 128, 128, 128]             256
             ReLU-10        [-1, 128, 128, 128]               0
           Conv2d-11        [-1, 128, 128, 128]         147,584
      BatchNorm2d-12        [-1, 128, 128, 128]             256
             ReLU-13        [-1, 128, 128, 128]               0
        MaxPool2d-14          [-1, 128, 64, 64]               0
           Conv2d-15          [-1, 256, 64, 64]         295,168
      BatchNorm2d-16          [-1, 256, 64, 64]             512
             ReLU-17          [-1, 256, 64, 64]               0
           Conv2d-18          [-1, 256, 64, 64]         590,080
      BatchNorm2d-19          [-1, 256, 64, 64]             512
             ReLU-20          [-1, 256, 64, 64]               0
        MaxPool2d-21          [-1, 256, 32, 32]               0
           Conv2d-22          [-1, 512, 32, 32]       1,180,160
      BatchNorm2d-23          [-1, 512, 32, 32]           1,024
             ReLU-24          [-1, 512, 32, 32]               0
           Conv2d-25          [-1, 512, 32, 32]       2,359,808
      BatchNorm2d-26          [-1, 512, 32, 32]           1,024
             ReLU-27          [-1, 512, 32, 32]               0
        MaxPool2d-28          [-1, 512, 16, 16]               0
           Conv2d-29         [-1, 1024, 16, 16]       4,719,616
      BatchNorm2d-30         [-1, 1024, 16, 16]           2,048
             ReLU-31         [-1, 1024, 16, 16]               0
           Conv2d-32         [-1, 1024, 16, 16]       9,438,208
      BatchNorm2d-33         [-1, 1024, 16, 16]           2,048
             ReLU-34         [-1, 1024, 16, 16]               0
         Upsample-35         [-1, 1024, 32, 32]               0
           Conv2d-36          [-1, 512, 32, 32]       7,078,400
             ReLU-37          [-1, 512, 32, 32]               0
           Conv2d-38          [-1, 512, 32, 32]       2,359,808
             ReLU-39          [-1, 512, 32, 32]               0
         Upsample-40          [-1, 512, 64, 64]               0
           Conv2d-41          [-1, 256, 64, 64]       1,769,728
             ReLU-42          [-1, 256, 64, 64]               0
           Conv2d-43          [-1, 256, 64, 64]         590,080
             ReLU-44          [-1, 256, 64, 64]               0
         Upsample-45        [-1, 256, 128, 128]               0
           Conv2d-46        [-1, 128, 128, 128]         442,496
             ReLU-47        [-1, 128, 128, 128]               0
           Conv2d-48        [-1, 128, 128, 128]         147,584
             ReLU-49        [-1, 128, 128, 128]               0
         Upsample-50        [-1, 128, 256, 256]               0
           Conv2d-51         [-1, 64, 256, 256]         110,656
             ReLU-52         [-1, 64, 256, 256]               0
           Conv2d-53         [-1, 64, 256, 256]          36,928
             ReLU-54         [-1, 64, 256, 256]               0
           Conv2d-55          [-1, 1, 256, 256]              65
================================================================
Total params: 31,385,729
Trainable params: 31,385,729
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.25
Forward/backward pass size (MB): 747.50
Params size (MB): 119.73
Estimated Total Size (MB): 867.48
----------------------------------------------------------------
----------------------------------------------------------------
The number of train set: 2064
The number of valid set: 240
----------------------------------------------------------------
Epoch 0/299
----------
LR 0.003
train: bce: 0.098407, dice: 0.990871, loss: 0.098407
val: bce: 0.019857, dice: 0.990236, loss: 0.019857
Validation loss decreased (inf --> 0.019857).  Saving best model ...
1m 19s
Epoch 1/299
----------
LR 0.003
train: bce: 0.019662, dice: 0.989639, loss: 0.019662
val: bce: 0.019172, dice: 0.988603, loss: 0.019172
Validation loss decreased (0.019857 --> 0.019172).  Saving best model ...
1m 21s
Epoch 2/299
----------
LR 0.003
train: bce: 0.019282, dice: 0.988713, loss: 0.019282
val: bce: 0.018730, dice: 0.988133, loss: 0.018730
Validation loss decreased (0.019172 --> 0.018730).  Saving best model ...
1m 21s
Epoch 3/299
----------
LR 0.003
train: bce: 0.019090, dice: 0.988203, loss: 0.019090
val: bce: 0.018581, dice: 0.987773, loss: 0.018581
Validation loss decreased (0.018730 --> 0.018581).  Saving best model ...
1m 20s
Epoch 4/299
----------
LR 0.003
train: bce: 0.018970, dice: 0.987911, loss: 0.018970
val: bce: 0.018487, dice: 0.987433, loss: 0.018487
Validation loss decreased (0.018581 --> 0.018487).  Saving best model ...
1m 19s
Epoch 5/299
----------
LR 0.003
train: bce: 0.018894, dice: 0.987735, loss: 0.018894
val: bce: 0.018398, dice: 0.986993, loss: 0.018398
Validation loss decreased (0.018487 --> 0.018398).  Saving best model ...
1m 20s
Epoch 6/299
----------
LR 0.003
train: bce: 0.018843, dice: 0.987550, loss: 0.018843
val: bce: 0.018282, dice: 0.986899, loss: 0.018282
Validation loss decreased (0.018398 --> 0.018282).  Saving best model ...
1m 19s
Epoch 7/299
----------
LR 0.003
train: bce: 0.018848, dice: 0.987407, loss: 0.018848
val: bce: 0.018268, dice: 0.987073, loss: 0.018268
Validation loss decreased (0.018282 --> 0.018268).  Saving best model ...
1m 20s
Epoch 8/299
----------
LR 0.003
train: bce: 0.018733, dice: 0.987254, loss: 0.018733
val: bce: 0.018225, dice: 0.986731, loss: 0.018225
Validation loss decreased (0.018268 --> 0.018225).  Saving best model ...
1m 19s
Epoch 9/299
----------
LR 0.003
train: bce: 0.018690, dice: 0.987043, loss: 0.018690
val: bce: 0.018170, dice: 0.985966, loss: 0.018170
Validation loss decreased (0.018225 --> 0.018170).  Saving best model ...
1m 18s
Epoch 10/299
----------
LR 0.003
train: bce: 0.018632, dice: 0.986851, loss: 0.018632
val: bce: 0.018157, dice: 0.986532, loss: 0.018157
Validation loss decreased (0.018170 --> 0.018157).  Saving best model ...
1m 19s
Epoch 11/299
----------
LR 0.003
train: bce: 0.018595, dice: 0.986743, loss: 0.018595
val: bce: 0.018060, dice: 0.986078, loss: 0.018060
Validation loss decreased (0.018157 --> 0.018060).  Saving best model ...
1m 20s
Epoch 12/299
----------
LR 0.003
train: bce: 0.018520, dice: 0.986445, loss: 0.018520
val: bce: 0.017965, dice: 0.985819, loss: 0.017965
Validation loss decreased (0.018060 --> 0.017965).  Saving best model ...
1m 21s
Epoch 13/299
----------
LR 0.003
train: bce: 0.018536, dice: 0.986523, loss: 0.018536
val: bce: 0.017924, dice: 0.985898, loss: 0.017924
Validation loss decreased (0.017965 --> 0.017924).  Saving best model ...
1m 20s
Epoch 14/299
----------
LR 0.003
train: bce: 0.018401, dice: 0.986176, loss: 0.018401
val: bce: 0.017827, dice: 0.984665, loss: 0.017827
Validation loss decreased (0.017924 --> 0.017827).  Saving best model ...
1m 20s
Epoch 15/299
----------
LR 0.003
train: bce: 0.018320, dice: 0.985714, loss: 0.018320
val: bce: 0.017696, dice: 0.985172, loss: 0.017696
Validation loss decreased (0.017827 --> 0.017696).  Saving best model ...
1m 20s
Epoch 16/299
----------
LR 0.003
train: bce: 0.018191, dice: 0.985450, loss: 0.018191
val: bce: 0.017511, dice: 0.983598, loss: 0.017511
Validation loss decreased (0.017696 --> 0.017511).  Saving best model ...
1m 19s
Epoch 17/299
----------
LR 0.003
train: bce: 0.017979, dice: 0.984647, loss: 0.017979
val: bce: 0.017266, dice: 0.982111, loss: 0.017266
Validation loss decreased (0.017511 --> 0.017266).  Saving best model ...
1m 21s
Epoch 18/299
----------
LR 0.003
train: bce: 0.017712, dice: 0.983435, loss: 0.017712
val: bce: 0.016961, dice: 0.981526, loss: 0.016961
Validation loss decreased (0.017266 --> 0.016961).  Saving best model ...
1m 21s
Epoch 19/299
----------
LR 0.003
train: bce: 0.017459, dice: 0.982107, loss: 0.017459
val: bce: 0.016635, dice: 0.977488, loss: 0.016635
Validation loss decreased (0.016961 --> 0.016635).  Saving best model ...
1m 21s
Epoch 20/299
----------
LR 0.003
train: bce: 0.017091, dice: 0.979997, loss: 0.017091
val: bce: 0.016017, dice: 0.974501, loss: 0.016017
Validation loss decreased (0.016635 --> 0.016017).  Saving best model ...
1m 21s
Epoch 21/299
----------
LR 0.003
train: bce: 0.016804, dice: 0.977688, loss: 0.016804
val: bce: 0.015664, dice: 0.972694, loss: 0.015664
Validation loss decreased (0.016017 --> 0.015664).  Saving best model ...
1m 21s
Epoch 22/299
----------
LR 0.003
train: bce: 0.016332, dice: 0.974804, loss: 0.016332
val: bce: 0.015280, dice: 0.968846, loss: 0.015280
Validation loss decreased (0.015664 --> 0.015280).  Saving best model ...
1m 19s
Epoch 23/299
----------
LR 0.003
train: bce: 0.015793, dice: 0.970769, loss: 0.015793
val: bce: 0.014597, dice: 0.961218, loss: 0.014597
Validation loss decreased (0.015280 --> 0.014597).  Saving best model ...
1m 18s
Epoch 24/299
----------
LR 0.003
train: bce: 0.015208, dice: 0.965456, loss: 0.015208
val: bce: 0.013798, dice: 0.955920, loss: 0.013798
Validation loss decreased (0.014597 --> 0.013798).  Saving best model ...
1m 17s
Epoch 25/299
----------
LR 0.003
train: bce: 0.014562, dice: 0.958471, loss: 0.014562
val: bce: 0.013018, dice: 0.942364, loss: 0.013018
Validation loss decreased (0.013798 --> 0.013018).  Saving best model ...
1m 16s
Epoch 26/299
----------
LR 0.003
train: bce: 0.014056, dice: 0.951090, loss: 0.014056
val: bce: 0.012343, dice: 0.936895, loss: 0.012343
Validation loss decreased (0.013018 --> 0.012343).  Saving best model ...
1m 16s
Epoch 27/299
----------
LR 0.003
train: bce: 0.013023, dice: 0.939798, loss: 0.013023
val: bce: 0.011289, dice: 0.918596, loss: 0.011289
Validation loss decreased (0.012343 --> 0.011289).  Saving best model ...
1m 18s
Epoch 28/299
----------
LR 0.003
train: bce: 0.012368, dice: 0.927107, loss: 0.012368
val: bce: 0.010642, dice: 0.906724, loss: 0.010642
Validation loss decreased (0.011289 --> 0.010642).  Saving best model ...
1m 18s
Epoch 29/299
----------
LR 0.003
train: bce: 0.011901, dice: 0.916440, loss: 0.011901
val: bce: 0.010569, dice: 0.893132, loss: 0.010569
Validation loss decreased (0.010642 --> 0.010569).  Saving best model ...
1m 17s
Epoch 30/299
----------
LR 0.003
train: bce: 0.011168, dice: 0.901799, loss: 0.011168
val: bce: 0.010736, dice: 0.843141, loss: 0.010736
EarlyStopping counter: 1 out of 15
1m 17s
Epoch 31/299
----------
LR 0.003
train: bce: 0.010951, dice: 0.893276, loss: 0.010951
val: bce: 0.009863, dice: 0.861152, loss: 0.009863
Validation loss decreased (0.010569 --> 0.009863).  Saving best model ...
1m 16s
Epoch 32/299
----------
LR 0.003
train: bce: 0.010277, dice: 0.881514, loss: 0.010277
val: bce: 0.008838, dice: 0.855447, loss: 0.008838
Validation loss decreased (0.009863 --> 0.008838).  Saving best model ...
1m 17s
Epoch 33/299
----------
LR 0.003
train: bce: 0.009627, dice: 0.866975, loss: 0.009627
val: bce: 0.008728, dice: 0.859032, loss: 0.008728
Validation loss decreased (0.008838 --> 0.008728).  Saving best model ...
1m 17s
Epoch 34/299
----------
LR 0.003
train: bce: 0.009317, dice: 0.855579, loss: 0.009317
val: bce: 0.007836, dice: 0.810577, loss: 0.007836
Validation loss decreased (0.008728 --> 0.007836).  Saving best model ...
1m 17s
Epoch 35/299
----------
LR 0.003
train: bce: 0.009274, dice: 0.853942, loss: 0.009274
val: bce: 0.007873, dice: 0.805377, loss: 0.007873
EarlyStopping counter: 1 out of 15
1m 17s
Epoch 36/299
----------
LR 0.003
train: bce: 0.008941, dice: 0.842654, loss: 0.008941
val: bce: 0.008192, dice: 0.818558, loss: 0.008192
EarlyStopping counter: 2 out of 15
1m 16s
Epoch 37/299
----------
LR 0.003
train: bce: 0.008622, dice: 0.832957, loss: 0.008622
val: bce: 0.010544, dice: 0.843765, loss: 0.010544
EarlyStopping counter: 3 out of 15
1m 16s
Epoch 38/299
----------
LR 0.003
train: bce: 0.008190, dice: 0.825229, loss: 0.008190
val: bce: 0.007440, dice: 0.801085, loss: 0.007440
Validation loss decreased (0.007836 --> 0.007440).  Saving best model ...
1m 16s
Epoch 39/299
----------
LR 0.003
train: bce: 0.008271, dice: 0.817766, loss: 0.008271
val: bce: 0.007302, dice: 0.781219, loss: 0.007302
Validation loss decreased (0.007440 --> 0.007302).  Saving best model ...
1m 16s
Epoch 40/299
----------
LR 0.003
train: bce: 0.007869, dice: 0.808014, loss: 0.007869
val: bce: 0.007312, dice: 0.776392, loss: 0.007312
EarlyStopping counter: 1 out of 15
1m 16s
Epoch 41/299
----------
LR 0.003
train: bce: 0.007605, dice: 0.796849, loss: 0.007605
val: bce: 0.007245, dice: 0.759884, loss: 0.007245
Validation loss decreased (0.007302 --> 0.007245).  Saving best model ...
1m 17s
Epoch 42/299
----------
LR 0.003
train: bce: 0.007718, dice: 0.793539, loss: 0.007718
val: bce: 0.006768, dice: 0.758887, loss: 0.006768
Validation loss decreased (0.007245 --> 0.006768).  Saving best model ...
1m 17s
Epoch 43/299
----------
LR 0.003
train: bce: 0.007415, dice: 0.791244, loss: 0.007415
val: bce: 0.006638, dice: 0.741593, loss: 0.006638
Validation loss decreased (0.006768 --> 0.006638).  Saving best model ...
1m 17s
Epoch 44/299
----------
LR 0.003
train: bce: 0.007236, dice: 0.778060, loss: 0.007236
val: bce: 0.007287, dice: 0.779561, loss: 0.007287
EarlyStopping counter: 1 out of 15
1m 16s
Epoch 45/299
----------
LR 0.003
train: bce: 0.007098, dice: 0.768548, loss: 0.007098
val: bce: 0.006613, dice: 0.704969, loss: 0.006613
Validation loss decreased (0.006638 --> 0.006613).  Saving best model ...
1m 17s
Epoch 46/299
----------
LR 0.003
train: bce: 0.007213, dice: 0.768897, loss: 0.007213
val: bce: 0.006226, dice: 0.727401, loss: 0.006226
Validation loss decreased (0.006613 --> 0.006226).  Saving best model ...
1m 18s
Epoch 47/299
----------
LR 0.003
train: bce: 0.006825, dice: 0.762310, loss: 0.006825
val: bce: 0.006368, dice: 0.733389, loss: 0.006368
EarlyStopping counter: 1 out of 15
1m 16s
Epoch 48/299
----------
LR 0.003
train: bce: 0.007054, dice: 0.754216, loss: 0.007054
val: bce: 0.006365, dice: 0.702096, loss: 0.006365
EarlyStopping counter: 2 out of 15
1m 17s
Epoch 49/299
----------
LR 0.003
train: bce: 0.006678, dice: 0.748655, loss: 0.006678
val: bce: 0.006628, dice: 0.735057, loss: 0.006628
EarlyStopping counter: 3 out of 15
1m 17s
Epoch 50/299
----------
LR 0.003
train: bce: 0.006441, dice: 0.738313, loss: 0.006441
val: bce: 0.006338, dice: 0.731467, loss: 0.006338
EarlyStopping counter: 4 out of 15
1m 16s
Epoch 51/299
----------
LR 0.003
train: bce: 0.006421, dice: 0.733331, loss: 0.006421
val: bce: 0.006256, dice: 0.709210, loss: 0.006256
EarlyStopping counter: 5 out of 15
1m 17s
Epoch 52/299
----------
LR 0.003
train: bce: 0.006494, dice: 0.728090, loss: 0.006494
val: bce: 0.006773, dice: 0.676674, loss: 0.006773
EarlyStopping counter: 6 out of 15
1m 17s
Epoch 53/299
----------
LR 0.0015
train: bce: 0.006104, dice: 0.716427, loss: 0.006104
val: bce: 0.005845, dice: 0.692540, loss: 0.005845
Validation loss decreased (0.006226 --> 0.005845).  Saving best model ...
1m 17s
Epoch 54/299
----------
LR 0.0015
train: bce: 0.005934, dice: 0.713021, loss: 0.005934
val: bce: 0.005851, dice: 0.669284, loss: 0.005851
EarlyStopping counter: 1 out of 15
1m 18s
Epoch 55/299
----------
LR 0.0015
train: bce: 0.005888, dice: 0.706070, loss: 0.005888
val: bce: 0.005679, dice: 0.665734, loss: 0.005679
Validation loss decreased (0.005845 --> 0.005679).  Saving best model ...
1m 18s
Epoch 56/299
----------
LR 0.0015
train: bce: 0.005912, dice: 0.705352, loss: 0.005912
val: bce: 0.005703, dice: 0.670796, loss: 0.005703
EarlyStopping counter: 1 out of 15
1m 19s
Epoch 57/299
----------
LR 0.0015
train: bce: 0.005655, dice: 0.690219, loss: 0.005655
val: bce: 0.005609, dice: 0.660134, loss: 0.005609
Validation loss decreased (0.005679 --> 0.005609).  Saving best model ...
1m 18s
Epoch 58/299
----------
LR 0.0015
train: bce: 0.005715, dice: 0.690410, loss: 0.005715
val: bce: 0.005763, dice: 0.653647, loss: 0.005763
EarlyStopping counter: 1 out of 15
1m 20s
Epoch 59/299
----------
LR 0.0015
train: bce: 0.005684, dice: 0.692399, loss: 0.005684
val: bce: 0.005786, dice: 0.665606, loss: 0.005786
EarlyStopping counter: 2 out of 15
1m 17s
Epoch 60/299
----------
LR 0.0015
train: bce: 0.005718, dice: 0.693501, loss: 0.005718
val: bce: 0.005664, dice: 0.661202, loss: 0.005664
EarlyStopping counter: 3 out of 15
1m 18s
Epoch 61/299
----------
LR 0.0015
train: bce: 0.005693, dice: 0.685946, loss: 0.005693
val: bce: 0.006266, dice: 0.604860, loss: 0.006266
EarlyStopping counter: 4 out of 15
1m 17s
Epoch 62/299
----------
LR 0.00075
train: bce: 0.005525, dice: 0.679349, loss: 0.005525
val: bce: 0.005458, dice: 0.613776, loss: 0.005458
Validation loss decreased (0.005609 --> 0.005458).  Saving best model ...
1m 19s
Epoch 63/299
----------
LR 0.00075
train: bce: 0.005414, dice: 0.670831, loss: 0.005414
val: bce: 0.005549, dice: 0.627482, loss: 0.005549
EarlyStopping counter: 1 out of 15
1m 17s
Epoch 64/299
----------
LR 0.00075
train: bce: 0.005429, dice: 0.669779, loss: 0.005429
val: bce: 0.005556, dice: 0.627093, loss: 0.005556
EarlyStopping counter: 2 out of 15
1m 18s
Epoch 65/299
----------
LR 0.00075
train: bce: 0.005459, dice: 0.670296, loss: 0.005459
val: bce: 0.005504, dice: 0.641196, loss: 0.005504
EarlyStopping counter: 3 out of 15
1m 18s
Epoch 66/299
----------
LR 0.00075
train: bce: 0.005395, dice: 0.671694, loss: 0.005395
val: bce: 0.005573, dice: 0.649702, loss: 0.005573
EarlyStopping counter: 4 out of 15
1m 17s
Epoch 67/299
----------
LR 0.00075
train: bce: 0.005265, dice: 0.659326, loss: 0.005265
val: bce: 0.005299, dice: 0.620938, loss: 0.005299
Validation loss decreased (0.005458 --> 0.005299).  Saving best model ...
1m 17s
Epoch 68/299
----------
LR 0.00075
train: bce: 0.005289, dice: 0.657380, loss: 0.005289
val: bce: 0.005467, dice: 0.632417, loss: 0.005467
EarlyStopping counter: 1 out of 15
1m 16s
Epoch 69/299
----------
LR 0.00075
train: bce: 0.005282, dice: 0.662010, loss: 0.005282
val: bce: 0.005260, dice: 0.602010, loss: 0.005260
Validation loss decreased (0.005299 --> 0.005260).  Saving best model ...
1m 16s
Epoch 70/299
----------
LR 0.00075
train: bce: 0.005279, dice: 0.659665, loss: 0.005279
val: bce: 0.005406, dice: 0.614024, loss: 0.005406
EarlyStopping counter: 1 out of 15
1m 16s
Epoch 71/299
----------
LR 0.00075
train: bce: 0.005302, dice: 0.653791, loss: 0.005302
val: bce: 0.005192, dice: 0.613180, loss: 0.005192
Validation loss decreased (0.005260 --> 0.005192).  Saving best model ...
1m 17s
Epoch 72/299
----------
LR 0.00075
train: bce: 0.005210, dice: 0.653028, loss: 0.005210
val: bce: 0.005467, dice: 0.638675, loss: 0.005467
EarlyStopping counter: 1 out of 15
1m 16s
Epoch 73/299
----------
LR 0.00075
train: bce: 0.005227, dice: 0.647758, loss: 0.005227
val: bce: 0.005405, dice: 0.615505, loss: 0.005405
EarlyStopping counter: 2 out of 15
1m 17s
Epoch 74/299
----------
LR 0.00075
train: bce: 0.005182, dice: 0.644306, loss: 0.005182
val: bce: 0.005254, dice: 0.628190, loss: 0.005254
EarlyStopping counter: 3 out of 15
1m 17s
Epoch 75/299
----------
LR 0.00075
train: bce: 0.005079, dice: 0.644173, loss: 0.005079
val: bce: 0.005122, dice: 0.598361, loss: 0.005122
Validation loss decreased (0.005192 --> 0.005122).  Saving best model ...
1m 18s
Epoch 76/299
----------
LR 0.00075
train: bce: 0.005107, dice: 0.644193, loss: 0.005107
val: bce: 0.005361, dice: 0.598399, loss: 0.005361
EarlyStopping counter: 1 out of 15
1m 17s
Epoch 77/299
----------
LR 0.00075
train: bce: 0.005122, dice: 0.639572, loss: 0.005122
val: bce: 0.005283, dice: 0.633097, loss: 0.005283
EarlyStopping counter: 2 out of 15
1m 17s
Epoch 78/299
----------
LR 0.000375
train: bce: 0.005065, dice: 0.645975, loss: 0.005065
val: bce: 0.005183, dice: 0.614223, loss: 0.005183
EarlyStopping counter: 3 out of 15
1m 18s
Epoch 79/299
----------
LR 0.000375
train: bce: 0.005054, dice: 0.640035, loss: 0.005054
val: bce: 0.005285, dice: 0.608978, loss: 0.005285
EarlyStopping counter: 4 out of 15
1m 20s
Epoch 80/299
----------
LR 0.000375
train: bce: 0.005005, dice: 0.641274, loss: 0.005005
val: bce: 0.004977, dice: 0.594869, loss: 0.004977
Validation loss decreased (0.005122 --> 0.004977).  Saving best model ...
1m 21s
Epoch 81/299
----------
LR 0.000375
train: bce: 0.005046, dice: 0.638538, loss: 0.005046
val: bce: 0.005282, dice: 0.590022, loss: 0.005282
EarlyStopping counter: 1 out of 15
1m 21s
Epoch 82/299
----------
LR 0.000375
train: bce: 0.004970, dice: 0.632502, loss: 0.004970
val: bce: 0.005596, dice: 0.630304, loss: 0.005596
EarlyStopping counter: 2 out of 15
1m 21s
Epoch 83/299
----------
LR 0.000375
train: bce: 0.004949, dice: 0.631536, loss: 0.004949
val: bce: 0.005095, dice: 0.610769, loss: 0.005095
EarlyStopping counter: 3 out of 15
1m 18s
Epoch 84/299
----------
LR 0.000375
train: bce: 0.004930, dice: 0.631519, loss: 0.004930
val: bce: 0.005173, dice: 0.607195, loss: 0.005173
EarlyStopping counter: 4 out of 15
1m 20s
Epoch 85/299
----------
LR 0.000375
train: bce: 0.004985, dice: 0.634216, loss: 0.004985
val: bce: 0.005187, dice: 0.588614, loss: 0.005187
EarlyStopping counter: 5 out of 15
1m 19s
Epoch 86/299
----------
LR 0.000375
train: bce: 0.004995, dice: 0.632656, loss: 0.004995
val: bce: 0.005308, dice: 0.605072, loss: 0.005308
EarlyStopping counter: 6 out of 15
1m 21s
Epoch 87/299
----------
LR 0.0001875
train: bce: 0.004950, dice: 0.628067, loss: 0.004950
val: bce: 0.005010, dice: 0.587162, loss: 0.005010
EarlyStopping counter: 7 out of 15
1m 19s
Epoch 88/299
----------
LR 0.0001875
train: bce: 0.004914, dice: 0.628543, loss: 0.004914
val: bce: 0.005193, dice: 0.595455, loss: 0.005193
EarlyStopping counter: 8 out of 15
1m 18s
Epoch 89/299
----------
LR 0.0001875
train: bce: 0.004951, dice: 0.632093, loss: 0.004951
val: bce: 0.005217, dice: 0.594922, loss: 0.005217
EarlyStopping counter: 9 out of 15
1m 19s
Epoch 90/299
----------
LR 0.0001875
train: bce: 0.004952, dice: 0.628284, loss: 0.004952
val: bce: 0.005090, dice: 0.590238, loss: 0.005090
EarlyStopping counter: 10 out of 15
1m 18s
Epoch 91/299
----------
LR 0.0001875
^CTraceback (most recent call last):
  File "train.py", line 316, in <module>
    main(args)
  File "train.py", line 211, in main
    model, metric_t, metric_v = train_model(model, optimizer_ft, scheduler, device, args.epochs, colon_dataloader)
  File "train.py", line 118, in train_model
Traceback (most recent call last):
    inputs = inputs.to(device)
  File "/home/chan/miniconda3/envs/torch/lib/python3.7/multiprocessing/queues.py", line 242, in _feed
    send_bytes(obj)
KeyboardInterrupt
  File "/home/chan/miniconda3/envs/torch/lib/python3.7/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/home/chan/miniconda3/envs/torch/lib/python3.7/multiprocessing/connection.py", line 404, in _send_bytes
    self._send(header + buf)
  File "/home/chan/miniconda3/envs/torch/lib/python3.7/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
(torch) chan@chan-desktop:~/Desktop/DLD1$ python test.py --model unet --metric-dataset-type undersample
----------------------------------------------------------------
The number of test set: 1189
----------------------------------------------------------------
----------
The Evaluation Starts ...
----------
The total samples: 1189
The average dice score is tensor([[0.8371]], device='cuda:0').
The number of tumor samples: 142
The average dice score of the slices which have tumor is tensor([[0.2202]], device='cuda:0').
The number of correct cases when the prediction predicts some poriton of the tumor: 65
The number of incorrect cases when the prediction predicts some poriton of the tumor: 15
The number of cases when the prediction predicts no tumor but it has tumor: 62
The number of non-tumor samples: 1047
The average dice score of the slices which have non-tumor is tensor([[0.9207]], device='cuda:0').
The number of cases when the prediction predicts no tumor when it has no tumor: 964
The number of cases when the prediction predicts tumor when it has no tumor: 83
0m 26s
