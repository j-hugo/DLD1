{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bittorchcondaa5fe8a2b8f6a42bbbf9cd8e18bcbab93",
   "display_name": "Python 3.7.7 64-bit ('torch': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Dataset and DataLoader\n",
    "\n",
    "2. Visualize one of outputs from data loader (greyscale)\n",
    "\n",
    "3. Unet model\n",
    "\n",
    "4. show training loop (but no train)\n",
    "\n",
    "5. load saved model (best one from our trained models)\n",
    "\n",
    "6. show test loop and run it\n",
    "\n",
    "7. show the result and visualization (greyscale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel\n",
    "import sys\n",
    "from glob import glob\n",
    "import argparse\n",
    "import json\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Convert NIfTI to Numpy\n",
    "\n",
    "First, we need to convert the original data to 2D slices because our model is for 2D image. We define *convert_to_npy()* function to convert 3D nifti image files to 2D slice of numpy files. Also, this function saves an image number, slice number, the exsistence of cancer, and subset as a JSON format. The *convert_to_npy()* function is at dataset.py. The JSON file is used to load a dataset.\n",
    "\n",
    "Second, we need to split the slices into the train set and test set. The *create_data_subsets()* function performs that task. You can choose how to split the data by examples(patients) or by slices and a split ratio between two sets. The *create_data_subsets()* function is at dataset.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_npy(data_path, path):\n",
    "    # load a path of image files and sort them\n",
    "    train_data_path = os.path.join(datapath, 'imagesTr')\n",
    "    label_data_path = os.path.join(datapath, 'labelsTr')\n",
    "    images = sorted(os.listdir(train_data_path))\n",
    "    labels = sorted(os.listdir(label_data_path))\n",
    "\n",
    "    # set the location to converted image files\n",
    "    image_saved_path =path +'npy_images/'\n",
    "    \n",
    "    # create a directory of the converted image files\n",
    "    try:\n",
    "        os.mkdir(image_saved_path)\n",
    "    except OSError:\n",
    "        print (\"Creation of the directory %s failed\" % image_saved_path)\n",
    "    else:\n",
    "        print (\"Successfully created the directory %s \" % image_saved_path)\n",
    "\n",
    "    # set the location to converted label files\n",
    "    label_saved_path = path + 'npy_labels/'\n",
    "    # create a directory of the converted label files\n",
    "    try:\n",
    "        os.mkdir(label_saved_path)\n",
    "    except OSError:\n",
    "        print (\"Creation of the directory %s failed\" % label_saved_path)\n",
    "    else:\n",
    "        print (\"Successfully created the directory %s \" % label_saved_path)\n",
    "        \n",
    "    data_index = {}\n",
    "\n",
    "    for img, label in zip(images,labels):\n",
    "        # Load 3D CT images\n",
    "        image_number = str(''.join(filter(str.isdigit, img))).zfill(3)\n",
    "        training_image = nibabel.load(os.path.join(train_data_path, img))\n",
    "        training_label = nibabel.load(os.path.join(label_data_path, label))\n",
    "\n",
    "        for k in range(training_label.shape[2]):\n",
    "            # axial cuts are made along the z axis (slice) \n",
    "            image_2d = np.array(training_image.get_fdata()[:, :, k], dtype='int16') # I checked: all values in the nifti files were integers, ranging from -1024 to approx 3000\n",
    "            label_2d = np.array(training_label.get_fdata()[:, :, k], dtype='uint8') # only contains 1s and 0s\n",
    "            slice_number = str(k).zfill(3)\n",
    "            slice_index = image_number+'_'+slice_number\n",
    "            \n",
    "            if len(np.unique(label_2d))!=1:\n",
    "              contains_cancer = True\n",
    "            else:\n",
    "              contains_cancer = False\n",
    "\n",
    "            data_index[slice_index] = {\n",
    "                'image': int(image_number),\n",
    "                'slice': int(slice_number),\n",
    "                'cancer': contains_cancer,\n",
    "                'subset': None\n",
    "            }\n",
    "\n",
    "            np.save((image_saved_path+'image_{}_{}.npy'.format(image_number,slice_number)), image_2d)\n",
    "            np.save((label_saved_path +'label_{}_{}.npy'.format(image_number,slice_number)), label_2d)\n",
    "            \n",
    "        print(f'Saved slices of image {image_number}')\n",
    "    \n",
    "    with open(path+\"data_index.json\", \"w\") as json_file:\n",
    "        json.dump(data_index,json_file)\n",
    "\n",
    "def create_data_subsets(path, split_on, split):\n",
    "    data_index_file = path+\"data_index.json\"\n",
    "    \n",
    "    with open(data_index_file) as json_file:\n",
    "        data_index = json.load(json_file)\n",
    "\n",
    "        if split_on == \"examples\":\n",
    "          image_index = [v['image'] for _, v in data_index.items()]\n",
    "          unique_images = set(image_index)\n",
    "          test_length = int(len(unique_images)*split)\n",
    "          test_images = random.sample(unique_images,test_length)\n",
    "          test_slices = [k for k,v in data_index.items() if v['image'] in test_images]\n",
    "          for k,_ in data_index.items():\n",
    "            if k in test_slices:\n",
    "              data_index[k]['subset'] = 'test'\n",
    "            else:\n",
    "              data_index[k]['subset'] = 'train'\n",
    "\n",
    "        if split_on == \"slices\":\n",
    "\n",
    "            cancer_slice_index = [k for k,v  in data_index.items() if v['cancer'] is True]\n",
    "            non_cancer_slice_index = [k for k, v in data_index.items() if v['cancer'] is False]\n",
    "            cancer_slice_length = len(cancer_slice_index)\n",
    "            non_cancer_slice_length = len(non_cancer_slice_index)\n",
    "            slice_length = cancer_slice_length + non_cancer_slice_length\n",
    "\n",
    "            proportion_cancer = cancer_slice_length/(non_cancer_slice_length+cancer_slice_length)\n",
    "            test_length = int(slice_length*split)\n",
    "            test_cancer_slices_length = int(test_length*proportion_cancer)\n",
    "            test_non_cancer_slices_length = test_length-test_cancer_slices_length\n",
    "\n",
    "            test_slices = [*random.sample(cancer_slice_index, test_cancer_slices_length),\n",
    "                           *random.sample(non_cancer_slice_index,test_non_cancer_slices_length)]\n",
    "\n",
    "            for k,_ in data_index.items():\n",
    "                if k in test_slices:\n",
    "                    data_index[k]['subset'] = 'test'\n",
    "                else:\n",
    "                    data_index[k]['subset'] = 'train'\n",
    "\n",
    "    with open(path+\"data_index_subsets.json\", \"w\") as json_file:\n",
    "        json.dump(data_index, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './Task10_Colon'\n",
    "path = './data/'\n",
    "split_on = \"slices\"\n",
    "split = 0.1\n",
    "\n",
    "convert_to_npy(data_path, path)\n",
    "create_data_subsets(path, split_on, split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dataset and Data Loader\n",
    "\n",
    "**1) Dataset**\n",
    "\n",
    "We creates a Custom Dataset; ColonDataset. It uses JSON file which created from *create_data_subsets()* function. *ColonDataset()* class is at **data_loading.py**.  \n",
    "\n",
    "We can set a sampling method. Otherwise, it uses the original dataset. Three sampling methods are undersample, oversample and only_tumor. Each sampling methods are defined as *get_undersample_files()*, *get_oversample_files()*, *get_only_tumor_files*, and *get_original_dataset()* at **data_loading.py**. \n",
    "\n",
    "When the data set is for train, it transforms the data set by resize, random crop, horizontal flipping, vertical flipping, and normalization. Otherwise, it transforms the data set by resize and normalization.\n",
    "\n",
    "After creating ColonDataset; we split the dataset into train and valid set. This logic is implemented in the *load_datasets()* function. The load_datasets() function is at *train.py*.\n",
    "\n",
    "**2) Data Loader**\n",
    "\n",
    "We use *DataLoader* from **pytorch**(torch.utils.data.DataLoader). At *load_dataloader()* function, we creates two data loaders of train and valid set and put both of them in the dictionary as 'train' and 'val'. This dictionary is going to use in the training loop later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_undersample_files(json_dir):\n",
    "  with open(json_dir) as json_file:\n",
    "    data_index = json.load(json_file)\n",
    "\n",
    "  index_with_cancer = [k for k,v in data_index.items() if (v['cancer'] == True) & (v['subset']=='train')]\n",
    "  index_no_cancer = [k for k,v in data_index.items() if (v['cancer'] == False) & (v['subset']=='train')]\n",
    "\n",
    "  # randomly draw indices from set of indices of slices without cancer\n",
    "  # same number of slices with and without cancer tissue\n",
    "  rand_index_no_cancer = random.choices(index_no_cancer,k=len(index_with_cancer))\n",
    "\n",
    "  image_files, label_files = [], []\n",
    "\n",
    "  # add file names of images and labels to list\n",
    "  for slice_index in rand_index_no_cancer:\n",
    "    image = 'image_'+slice_index + '.npy'\n",
    "    label = 'label_'+slice_index + '.npy'\n",
    "    image_files.append(image)\n",
    "    label_files.append(label)\n",
    "\n",
    "  for slice_index in index_with_cancer:\n",
    "    image = 'image_'+slice_index + '.npy'\n",
    "    label = 'label_'+slice_index+ '.npy'\n",
    "    image_files.append(image)\n",
    "    label_files.append(label)\n",
    "  \n",
    "  return(image_files,label_files)\n",
    "\n",
    "# oversample\n",
    "def get_oversample_files(json_dir):\n",
    "\n",
    "  with open(json_dir) as json_file:\n",
    "    data_index = json.load(json_file)\n",
    "\n",
    "  index_with_cancer = [k for k,v in data_index.items() if (v['cancer'] == True) & (v['subset']=='train')]\n",
    "  index_no_cancer = [k for k,v in data_index.items() if (v['cancer'] == False) & (v['subset']=='train')]\n",
    "\n",
    "  # randomly draw indices from set of indices of slices without cancer\n",
    "  # same number of slices with and without cancer tissue\n",
    "  rand_index_with_cancer = random.choices(index_with_cancer,k=len(index_no_cancer))\n",
    "\n",
    "  image_files, label_files = [], []\n",
    "\n",
    "  # add file names of images and labels to list\n",
    "\n",
    "  for slice_index in rand_index_with_cancer:\n",
    "    image = 'image_'+slice_index + '.npy'\n",
    "    label = 'label_'+slice_index + '.npy'\n",
    "    image_files.append(image)\n",
    "    label_files.append(label)\n",
    "\n",
    "  for slice_index in index_no_cancer:\n",
    "    image = 'image_'+slice_index + '.npy'\n",
    "    label = 'label_'+slice_index+ '.npy'\n",
    "    image_files.append(image)\n",
    "    label_files.append(label)\n",
    "  \n",
    "  return(image_files,label_files)\n",
    "\n",
    "# only_tumor_files() returns a list of all files that contain cancer tissue.\n",
    "# no files without cancer tissue will be returned\n",
    "def get_only_tumor_files(json_dir):\n",
    "  with open(json_dir) as json_file:\n",
    "    data_index = json.load(json_file)\n",
    "\n",
    "  index_with_cancer = [k for k,v in data_index.items() if (v['cancer'] == True) & (v['subset']=='train')]\n",
    "\n",
    "  image_files, label_files = [], []\n",
    "\n",
    "  # add file names of images and labels to list\n",
    "  for slice_index in index_with_cancer:\n",
    "    image = 'image_'+slice_index + '.npy'\n",
    "    label = 'label_'+slice_index+ '.npy'\n",
    "    image_files.append(image)\n",
    "    label_files.append(label)\n",
    "  \n",
    "  return(image_files,label_files)\n",
    "\n",
    "# get_original_dataset() returns a dataset without any sampling method\n",
    "def get_original_dataset(json_dir, test):\n",
    "    with open(json_dir) as json_file:\n",
    "        data_index = json.load(json_file)\n",
    "\n",
    "    if test is True:\n",
    "        file_index = [k for k, v in data_index.items() if v['subset'] == 'test']\n",
    "    else:\n",
    "        file_index = [k for k,v in data_index.items() if v['subset'] == 'train']\n",
    "\n",
    "    image_files, label_files = [], []\n",
    "\n",
    "    # add file names of images and labels to list\n",
    "    for slice_index in file_index:\n",
    "        image = 'image_' + slice_index + '.npy'\n",
    "        label = 'label_' + slice_index + '.npy'\n",
    "        image_files.append(image)\n",
    "        label_files.append(label)\n",
    "\n",
    "    return(image_files, label_files)\n",
    "\n",
    "# dataset class for primary colon cancer dataset\n",
    "class ColonDataset(Dataset):\n",
    "    \"\"\"Colon Cancer dataset.\"\"\"\n",
    "    def __init__(self, image_dir, label_dir, json_dir, image_size, torch_transform, balance_dataset=None, test=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_dir: Path to image folder.\n",
    "            label_dir: Path to label folder.\n",
    "            csv_dir: Path to csv file, which gives information whether slice contains annotated cancer pixels.\n",
    "            balance_dataset (optional): options to create a dataset with balanced numbers of slices\n",
    "                containing cancer tissue or not containing cancer\n",
    "                'oversample': uniformly draws samples from minority class to reach equal size\n",
    "                'undersample': uniformly draws samples from majority class to reach equal size\n",
    "                'only_tumor': only includes slices with cancer tissue\n",
    "                None: no balance method is applied\n",
    "        \"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.json_dir = json_dir\n",
    "        self.image_size = image_size\n",
    "        self.test = test\n",
    "        self.balance_dataset = balance_dataset\n",
    "        self.torch_transform = torch_transform\n",
    "        if self.test is None:\n",
    "            if self.balance_dataset == \"undersample\":\n",
    "                self.image_files, self.label_files = get_undersample_files(self.json_dir)\n",
    "            if self.balance_dataset == \"oversample\":\n",
    "                self.image_files, self.label_files = get_oversample_files(self.json_dir)\n",
    "            if self.balance_dataset == 'only_tumor':\n",
    "                self.image_files, self.label_files = only_tumor_files(self.json_dir)\n",
    "        if (self.balance_dataset is None) or (self.test is True):\n",
    "            self.image_files, self.label_files = get_original_dataset(self.json_dir, self.test)\n",
    "\n",
    "    def __len__(self):\n",
    "      return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        image_path = os.path.join(self.image_dir,\n",
    "                                  self.image_files[idx])\n",
    "        label_path = os.path.join(self.label_dir,\n",
    "                                  self.label_files[idx])\n",
    "        \n",
    "        image = np.load(image_path)\n",
    "        label = np.load(label_path)\n",
    "        if self.torch_transform:\n",
    "            x, y = self.transform(image, label)\n",
    "\n",
    "        return [x, y]\n",
    "\n",
    "    def transform(self, image, label):\n",
    "      # to PIL\n",
    "      image = PIL.Image.fromarray(image)\n",
    "      label = PIL.Image.fromarray(label)\n",
    "\n",
    "      # Resize\n",
    "      if self.test == None:\n",
    "        image = TF.resize(image, size=(self.image_size+44, self.image_size+44))\n",
    "        label = TF.resize(label, size=(self.image_size+44, self.image_size+44))\n",
    "      else:\n",
    "        image = TF.resize(image, size=(self.image_size, self.image_size))\n",
    "        label = TF.resize(label, size=(self.image_size, self.image_size))\n",
    "\n",
    "      # Random crop\n",
    "      if self.test == None:\n",
    "        i, j, h, w = transforms.RandomCrop.get_params(\n",
    "            image, output_size=(self.image_size, self.image_size))\n",
    "        image = TF.crop(image, i, j, h, w)\n",
    "        label = TF.crop(label, i, j, h, w)\n",
    "\n",
    "      # Random horizontal flipping\n",
    "      if self.test == None:\n",
    "        if random.random() > 0.5:\n",
    "            image = TF.hflip(image)\n",
    "            label = TF.hflip(label)\n",
    "\n",
    "      # Random vertical flipping\n",
    "      if self.test == None:\n",
    "        if random.random() > 0.5:\n",
    "            image = TF.vflip(image)\n",
    "            label = TF.vflip(label)\n",
    "\n",
    "      # Transform to tensor\n",
    "      image = torch.from_numpy(np.array(image)) # to_tensor: /opt/conda/conda-bld/pytorch_1587428094786/work/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. \n",
    "      image = image.unsqueeze(0).type(torch.FloatTensor)\n",
    "      label = torch.from_numpy(np.array(np.expand_dims(label, 0))).type(torch.FloatTensor)\n",
    "      # Normalize\n",
    "      image = TF.normalize(image, mean=(-531.28,), std=(499.68,))\n",
    "\n",
    "\n",
    "      return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets(split_ratio, img_path, label_path, json_file, image_size, transforms. dataset_type):\n",
    "    \"\"\"load the dataset (ColonDataset) and split the dataset into train and validation set   \n",
    "\n",
    "    Return:\n",
    "        train_dataset: a dataset for training\n",
    "        val_dataset: a dataset for validation\n",
    "    \"\"\"\n",
    "    dataset = ColonDataset(\n",
    "        image_dir=img_path,\n",
    "        label_dir=label_paths,\n",
    "        json_dir= json_file,\n",
    "        image_size=image_size,\n",
    "        torch_transform=transforms,\n",
    "        balance_dataset=dataset_type\n",
    "    )\n",
    "    # determine train and validation set size and split randomly\n",
    "    train_size = int(split_ratio*len(dataset))\n",
    "    val_size = len(dataset)-train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "def load_dataloader(batch_size, train, valid):\n",
    "    \"\"\"load a dataloader using train dataset and validation dataset (ColonDataset)\n",
    "\n",
    "    Args:\n",
    "        args: the object which store arguments from the parser\n",
    "        train: train dataset (ColonDataset)\n",
    "        valid: validation dataset (ColonDataset)\n",
    "        \n",
    "    Return:\n",
    "        dataloader: a dataloader for training \n",
    "    \"\"\"\n",
    "    dataloader = {\n",
    "       'train': DataLoader(train, shuffle=True, batch_size=batch_size, num_workers=4),\n",
    "        'val': DataLoader(valid, shuffle=True, batch_size=batch_size, num_workers=4)\n",
    "    }\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = 0.9\n",
    "batch_size = 12\n",
    "img_path = \"./data/npy_images\"\n",
    "label+path = \"./data/npy_labels\"\n",
    "json_file = \"./data/data_index_subsets.json\"\n",
    "transforms = True\n",
    "dataset_type = \"upsample\"\n",
    "image_size = 256\n",
    "\n",
    "train, valid = load_datasets(split_ratio, img_path, label_path, json_file, image_size, transforms. dataset_type):\n",
    "train_dataloader = load_dataloader(batch_size, train, valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Visualize images from the first batch of the data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}